{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Required dependencies\n",
    "# 1. NLTK\n",
    "# 2. Gensim for word2vec\n",
    "# 3. Keras with tensorflow/theano backend\n",
    "\n",
    "#=======================\n",
    "import json\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Reshape, Concatenate, Activation, LeakyReLU, Lambda\n",
    "from gensim.test.utils import common_texts, get_tmpfile #10-11\n",
    "import gc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from math import exp, log, e\n",
    "import tensorflow as tf\n",
    "from keras import metrics\n",
    "from IPython.display import Image\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "#========================\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, Masking, TimeDistributed, Bidirectional\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils, plot_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix, mean_squared_error\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse = '/data/projects/eclipse_activity.json'\n",
    "chrome = '/data/projects/chromium_activity.json'\n",
    "firefox = '/data/projects/firefox_activity.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================================\n",
    "# Initializing Hyper parameter\n",
    "#========================================================================================\n",
    "#1. Word2vec parameters\n",
    "min_word_frequency_word2vec = 30\n",
    "embed_size_word2vec = 200\n",
    "context_window_word2vec = 5\n",
    "combined_word2vec = True\n",
    "\n",
    "#2. Classifier hyperparameters\n",
    "numCV = 10\n",
    "max_sentence_len = 150\n",
    "min_sentence_length = 15\n",
    "max_his_len = 10\n",
    "max_activity_len = 10\n",
    "min_activity_len = 4\n",
    "batch_size = 32\n",
    "label_num=100\n",
    "class_std = 3 # 1=52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30428\n",
      "100001\n",
      "2015-11-10 22:04:19\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Preprocess 3 (ACTIVITY-BASED -> DATE-BASED)\n",
    "#==============================================================================\n",
    "paths = [chrome]#, chrome, firefox]\n",
    "data_name = {eclipse:'eclipse', chrome:'chrome', firefox:'firefox'}\n",
    "\n",
    "all_data_word2vec = []\n",
    "all_history_word2vec = []\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'had', 'having', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',  'so', 'than', 'too', 'very', 's', 't',  'just', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',  'ma']\n",
    "stop_words += list(string.ascii_lowercase)\n",
    "stop_words = set(stop_words)\n",
    "noise_day = 3\n",
    "accumulate=False\n",
    "limit_day = 100\n",
    "max_activity_len = 10\n",
    "\n",
    "cut_workday_fixingtime = False\n",
    "cut_workday = False\n",
    "cut_activity_len = True  # many-to-many\n",
    "alpha = 2\n",
    "beta = 0.2\n",
    "recentday = 3 # all_recentday\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, encoding='utf-8') as data_file:\n",
    "        data = json.loads(data_file.read(), strict=False)\n",
    "    all_data = []\n",
    "    all_history = []\n",
    "    all_time = []\n",
    "    #all_stream = []\n",
    "    all_bugid = []\n",
    "    \n",
    "    all_workday = []\n",
    "    all_recentday = []\n",
    "    all_openday = []\n",
    "    all_activitycnt = []\n",
    "    all_cc = []\n",
    "    all_writer = []\n",
    "    all_commcnt = []\n",
    "    all_hiscnt = []\n",
    "    all_wordcnt = []\n",
    "    prev_data = None\n",
    "    same_cnt = 0\n",
    "    for item in data:\n",
    "        if path==eclipse and int(item['closed'].split('-')[0])<2013:\n",
    "            continue\n",
    "        if len(item['activity']) != len(item['days']):\n",
    "            print(item['reportNum'],'error!')\n",
    "            continue\n",
    "        if len(all_time)>=100000: break\n",
    "        if len(item['days']) == 0 or item['days'][0] >= limit_day or item['days'][-1]>noise_day:\n",
    "            continue\n",
    "        current_title = item['title'].replace('\\r', ' ')\n",
    "        current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)\n",
    "        current_title = current_title.lower()\n",
    "        current_title = re.sub(r\"[!#$%&'()*+,./:;<=>?@\\^_`{|}~-]\", ' ', current_title)\n",
    "        current_title_tokens = nltk.word_tokenize(current_title)\n",
    "        current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]\n",
    "        all_data_word2vec.append(current_title_filter)\n",
    "        unique_date = [e[1].split(\" \")[0] for e in item['activity']]\n",
    "        unique_date = sorted(list(set(unique_date)))\n",
    "        if cut_activity_len and len(unique_date)>max_activity_len:\n",
    "            continue\n",
    "        if cut_workday:\n",
    "            if len(unique_date)<alpha:\n",
    "                continue\n",
    "        if cut_workday_fixingtime:\n",
    "            if len(unique_date)/(item['days'][0]+1) < beta:\n",
    "                continue\n",
    "        curr_activityCnt = 0\n",
    "        curr_workday = 0\n",
    "        #curr_stream = [0]*limit_day\n",
    "        \n",
    "        curr_data_list = []\n",
    "        curr_history_list = []\n",
    "        for j,date in enumerate(unique_date):\n",
    "            current_desc = \" \"\n",
    "            curr_his = \" \"\n",
    "            curr_workday +=1\n",
    "            this_activityCnt = 0\n",
    "            total_act = 0\n",
    "            comm_cnt = 0\n",
    "            his_cnt = 0 \n",
    "            tmp_cc = []\n",
    "            for idx, x in enumerate(item['activity']):\n",
    "                if x[1].split(' ')[0] > date:\n",
    "                    break   \n",
    "                if not accumulate:\n",
    "                    if x[1].split(' ')[0] < date:\n",
    "                        continue  \n",
    "                if x[2]!=\"\":\n",
    "                    total_act+=1\n",
    "                    comm_cnt +=1\n",
    "                if x[3]!=\"\":\n",
    "                    total_act+=1\n",
    "                    his_cnt +=1\n",
    "                aft_x0 = re.sub(r'\\b(Comment)(\\s+)(\\d+)', r'\\1\\3', x[0])\n",
    "                aft_x0 = re.sub(r'@(.+)\\.(.+)', '', aft_x0)\n",
    "                if aft_x0.split()[0]=='Reported' or 'Description' in x[0]:\n",
    "                    if aft_x0.split()[0]=='Reported':\n",
    "                        reporter = aft_x0.split()[2]\n",
    "                    else:\n",
    "                        reporter = x[0].split()[0]\n",
    "                    #print(reporter)\n",
    "                if path==firefox:\n",
    "                    cc = x[0].split()[0].strip(', ')\n",
    "                elif path ==chrome:\n",
    "                    cc = aft_x0.split()[2].strip()\n",
    "                elif path==eclipse:\n",
    "                    cc= x[0].split()[0]\n",
    "                tmp_cc.append(cc)\n",
    "                current_desc += aft_x0+' '+x[2]+' '                \n",
    "                curr_his += x[3].replace(':',' : ').replace('>', ' > ') + ' '\n",
    "                curr_time = item['days'][idx]\n",
    "                curr_commentDate = item['commentDate'][idx]\n",
    "                curr_activityCnt+=1\n",
    "                this_activityCnt+=1\n",
    "            curr_active = [0]*recentday\n",
    "            for idx, x in enumerate(item['activity']):\n",
    "                for p in range(recentday,0,-1):\n",
    "                    if item['days'][idx]-curr_time==p:\n",
    "                        curr_active[-p] = 1\n",
    "                     \n",
    "            current_desc = current_desc.replace('\\r', ' ')\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n",
    "            current_desc = re.sub(r'(\\w*)0x\\w+', ' ', current_desc)\n",
    "            current_desc = re.sub(r'[^(A-Za-z0-9\\s)]+', ' ', current_desc) # Solaris-specific >> solaris, specific\n",
    "            current_desc = re.sub(r\"(\\s+)\\d+\", \" \", current_desc)\n",
    "            current_desc = current_desc.lower()\n",
    "            current_desc_tokens = nltk.word_tokenize(current_desc)\n",
    "            current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n",
    "            curr_data = current_title_filter + current_desc_filter\n",
    "            #curr_data = current_desc_filter\n",
    "            curr_data = list(filter(None, curr_data))\n",
    "            curr_data = [w for w in curr_data if not w in stop_words]\n",
    "            if curr_data!=[]:\n",
    "                all_data_word2vec.append(curr_data)\n",
    "            \n",
    "            curr_his = curr_his.replace('✿','').replace('(',' ').replace(')',' ')\n",
    "            curr_his = re.sub(r'@(.+)\\.(.+)', '', curr_his)\n",
    "            curr_his = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', curr_his)\n",
    "            curr_his = re.sub(r'Attachment(\\s+)[#][0-9]+(\\s+)-', '', curr_his)\n",
    "            curr_his = curr_his.lower()\n",
    "            curr_his_tokens = nltk.word_tokenize(curr_his)\n",
    "            curr_his_filter = [word.strip(string.punctuation) for word in curr_his_tokens]\n",
    "            #curr_his_tokens = curr_his.split()\n",
    "            #curr_his_filter = [word for word in curr_his_tokens if not all(ch in string.punctuation for ch in list(word))]\n",
    "            curr_history = list(filter(None, curr_his_filter))\n",
    "            curr_history = [w for w in curr_history if not w in stop_words]\n",
    "            if curr_history!=[]:\n",
    "                all_data_word2vec.append(curr_history)\n",
    "                \n",
    "            #curr_stream[curr_commentDate] = this_activityCnt\n",
    "            #for x in range(curr_commentDate):\n",
    "            #    if curr_stream[x]== 0:\n",
    "            #        curr_stream[x] = -1\n",
    "            curr_data_list.append(curr_data)\n",
    "            #prev_data=curr_data\n",
    "            curr_history_list.append(curr_history)\n",
    "            all_time.append(curr_time)\n",
    "            #all_stream.append(list(curr_stream))\n",
    "            all_bugid.append(item['reportNum'])\n",
    "            \n",
    "            all_commcnt.append(comm_cnt)\n",
    "            all_hiscnt.append(his_cnt)\n",
    "            all_wordcnt.append(len(curr_history)+len(curr_data))\n",
    "            all_workday.append(curr_workday)\n",
    "            all_recentday.append(curr_active)\n",
    "            all_openday.append(curr_commentDate)\n",
    "            all_activitycnt.append(total_act)\n",
    "            tmp_cc = set(tmp_cc)\n",
    "            all_cc.append(len(set(tmp_cc)))\n",
    "            \n",
    "            min_writer = 0\n",
    "            if path == firefox:\n",
    "                tmp_writer = [0,0,0]\n",
    "                if 'reporter' in curr_data:\n",
    "                    tmp_writer[0]=1\n",
    "                    min_writer+=1\n",
    "                if 'assignee' in curr_data:\n",
    "                    tmp_writer[1]=1\n",
    "                    min_writer+=1\n",
    "                if len(tmp_cc)>min_writer:\n",
    "                    tmp_writer[2]=1\n",
    "            elif path==chrome or path ==eclipse:\n",
    "                tmp_writer = [0,0]\n",
    "                if reporter in tmp_cc:\n",
    "                    tmp_writer[0]=1\n",
    "                    min_writer+=1\n",
    "                if len(tmp_cc)>min_writer:\n",
    "                    tmp_writer[1]=1\n",
    "            all_writer.append(tmp_writer)\n",
    "        all_data.append(curr_data_list)\n",
    "        all_history.append(curr_history_list)\n",
    " \n",
    "    del data\n",
    "    gc.collect()\n",
    "print(len(all_data))\n",
    "print(len(all_time))\n",
    "print(item['closed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2vec\n",
    "\n",
    "if combined_word2vec:\n",
    "    if path == chrome:\n",
    "        wordvec_path = '/data/word2vec/chrome_combined200.model'\n",
    "    elif path ==eclipse:\n",
    "        wordvec_path = '/data/word2vec/eclipse_combined200.model'\n",
    "    elif path ==firefox:\n",
    "        wordvec_path = '/data/word2vec/firefox_combined200.model'\n",
    "    wordvec_model = Word2Vec.load(wordvec_path)\n",
    "    vocabulary = wordvec_model.wv.vocab\n",
    "else: \n",
    "    wordvec_path_1 = '/data/word2vec/newProject_all_data_word2vec.model'\n",
    "    wordvec_path_2 = '/data/word2vec/newProject_all_history_word2vec.model'\n",
    "    wordvec_model_1 = Word2Vec.load(wordvec_path_1)\n",
    "    wordvec_model_2 = Word2Vec.load(wordvec_path_2)\n",
    "    vocabulary_1 = wordvec_model_1.wv.vocab\n",
    "    vocabulary_2 = wordvec_model_2.wv.vocab\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Lambda\n",
    "from keras.layers.merge import add\n",
    "\n",
    "def make_residual_lstm_layers(input, rnn_width, rnn_depth, rnn_dropout):\n",
    "    \"\"\"\n",
    "    The intermediate LSTM layers return sequences, while the last returns a single element.\n",
    "    The input is also a sequence. In order to match the shape of input and output of the LSTM\n",
    "    to sum them we can do it only for all layers but the last.\n",
    "    \"\"\"\n",
    "    x = input\n",
    "    x = Masking(mask_value=0)(x)\n",
    "    cnt = 1\n",
    "    for i in range(rnn_depth):\n",
    "        return_sequences = i < rnn_depth # - 1 # 마지막도 return_sequences\n",
    "        x_rnn = LSTM(rnn_width, return_sequences=return_sequences, name='lstm'+str(cnt))(x) #recurrent_dropout=rnn_dropout, dropout=rnn_dropout,\n",
    "        cnt+=1\n",
    "        if return_sequences:\n",
    "            # Intermediate layers return sequences, input is also a sequence.\n",
    "            if i > 0 or input.shape[-1] == rnn_width:\n",
    "                x = add([x, x_rnn])\n",
    "            else:\n",
    "                # Note that the input size and RNN output has to match, due to the sum operation.\n",
    "                # If we want different rnn_width, we'd have to perform the sum from layer 2 on.\n",
    "                x = x_rnn\n",
    "            if i%2==1:\n",
    "                x = BatchNormalization(axis=1, name='batch'+str(cnt))(x)\n",
    "                cnt+=1\n",
    "        else:\n",
    "            # Last layer does not return sequences, just the last element\n",
    "            # so we select only the last element of the previous output.\n",
    "            def slice_last(x):\n",
    "                return x[..., -1, :]\n",
    "            x = add([Lambda(slice_last)(x), x_rnn])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Class-embedding Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length:  30428\n",
      "train_balancing False\n",
      "CV 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yksi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:193: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/yksi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:206: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yksi/.conda/envs/RTDB/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yksi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:311: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ou...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************* M3-RNN *******************\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 150, 200)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 10, 200)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "forwards_1 (LSTM)               (None, 256)          467968      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwords_1 (LSTM)              (None, 256)          467968      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "forwards_2 (LSTM)               (None, 32)           29824       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwards_2 (LSTM)              (None, 32)           29824       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           forwards_1[0][0]                 \n",
      "                                                                 backwords_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64)           0           forwards_2[0][0]                 \n",
      "                                                                 backwards_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_1 (Dropout)            (None, 512)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_2 (Dropout)            (None, 64)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 28)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 604)          0           after_dp_1[0][0]                 \n",
      "                                                                 after_dp_2[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_1 (Dense)          (None, 380)          229900      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 380)          0           middle_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_2 (Dense)          (None, 450)          171450      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 450)          0           middle_dense_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_3 (Dense)          (None, 200)          90200       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 7)            1407        middle_dense_3[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,488,541\n",
      "Trainable params: 1,488,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 46496 samples, validate on 8206 samples\n",
      "Epoch 1/20\n",
      "46496/46496 [==============================] - 778s 17ms/step - loss: 1.7642 - acc: 0.3122 - val_loss: 1.7335 - val_acc: 0.3195\n",
      "Epoch 2/20\n",
      "46496/46496 [==============================] - 773s 17ms/step - loss: 1.7118 - acc: 0.3358 - val_loss: 1.7174 - val_acc: 0.3354\n",
      "Epoch 3/20\n",
      "46496/46496 [==============================] - 781s 17ms/step - loss: 1.6794 - acc: 0.3488 - val_loss: 1.7017 - val_acc: 0.3401\n",
      "Epoch 4/20\n",
      "46496/46496 [==============================] - 786s 17ms/step - loss: 1.6445 - acc: 0.3649 - val_loss: 1.6925 - val_acc: 0.3439\n",
      "Epoch 5/20\n",
      "46496/46496 [==============================] - 780s 17ms/step - loss: 1.5989 - acc: 0.3830 - val_loss: 1.6871 - val_acc: 0.3480\n",
      "Epoch 6/20\n",
      "46496/46496 [==============================] - 777s 17ms/step - loss: 1.5487 - acc: 0.4070 - val_loss: 1.7002 - val_acc: 0.3440\n",
      " ********* emb7-M3 Learning time: 4676sec ********* \n",
      "Saved model to disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yksi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:514: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/yksi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:527: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************* M3 - Embedding RNN *******************\n",
      "Total test accuracy : 31.51\n",
      "\n",
      "Confusion Matrix\n",
      "[[1558  154    5   38  143   38   75]\n",
      " [ 532  257   13   97  292   88  131]\n",
      " [ 263  161    9   60  218   93  105]\n",
      " [ 255  162    4   92  346  126  184]\n",
      " [ 236  168   13  106  541  221  270]\n",
      " [ 124   67    9   76  348  159  212]\n",
      " [ 140   71    9   73  392  218  299]]\n",
      "\n",
      "f1_score: 31.51\n",
      "precision_score: 31.51\n",
      "recall_score: 31.51\n",
      "\n",
      "31.51 31.51 31.51 31.51\n",
      "\n",
      "\n",
      "Total length:  30428\n",
      "train_balancing False\n",
      "CV 7\n",
      "\n",
      "\n",
      "******************* M3-RNN *******************\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 150, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 10, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, 150, 200)     0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_4 (Masking)             (None, 10, 200)      0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "forwards_1 (LSTM)               (None, 256)          467968      masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwords_1 (LSTM)              (None, 256)          467968      masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "forwards_2 (LSTM)               (None, 32)           29824       masking_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwards_2 (LSTM)              (None, 32)           29824       masking_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           forwards_1[0][0]                 \n",
      "                                                                 backwords_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64)           0           forwards_2[0][0]                 \n",
      "                                                                 backwards_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_1 (Dropout)            (None, 512)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_2 (Dropout)            (None, 64)           0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 28)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 604)          0           after_dp_1[0][0]                 \n",
      "                                                                 after_dp_2[0][0]                 \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_1 (Dense)          (None, 380)          229900      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 380)          0           middle_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_2 (Dense)          (None, 450)          171450      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 450)          0           middle_dense_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_3 (Dense)          (None, 200)          90200       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 7)            1407        middle_dense_3[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,488,541\n",
      "Trainable params: 1,488,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 54360 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54360/54360 [==============================] - 930s 17ms/step - loss: 1.7603 - acc: 0.3126 - val_loss: 1.7321 - val_acc: 0.3178\n",
      "Epoch 2/20\n",
      "54360/54360 [==============================] - 911s 17ms/step - loss: 1.7109 - acc: 0.3354 - val_loss: 1.7113 - val_acc: 0.3342\n",
      "Epoch 3/20\n",
      "54360/54360 [==============================] - 916s 17ms/step - loss: 1.6800 - acc: 0.3488 - val_loss: 1.6988 - val_acc: 0.3395\n",
      "Epoch 4/20\n",
      "54360/54360 [==============================] - 927s 17ms/step - loss: 1.6454 - acc: 0.3645 - val_loss: 1.6941 - val_acc: 0.3486\n",
      "Epoch 5/20\n",
      "54360/54360 [==============================] - 936s 17ms/step - loss: 1.6036 - acc: 0.3797 - val_loss: 1.6967 - val_acc: 0.3458\n",
      " ********* emb7-M3 Learning time: 4622sec ********* \n",
      "Saved model to disk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******************* M3 - Embedding RNN *******************\n",
      "Total test accuracy : 32.39\n",
      "\n",
      "Confusion Matrix\n",
      "[[1603  116    0   30  142    9   61]\n",
      " [ 653  258    3   74  305   24   91]\n",
      " [ 322  162    4   63  220   26   90]\n",
      " [ 325  153    3   84  377   38  142]\n",
      " [ 328  209    5  104  574   46  227]\n",
      " [ 165  108    3   61  355   57  179]\n",
      " [ 152  126    2   64  432   42  312]]\n",
      "\n",
      "f1_score: 32.39\n",
      "precision_score: 32.39\n",
      "recall_score: 32.39\n",
      "\n",
      "32.39 32.39 32.39 32.39\n",
      "\n",
      "\n",
      "Total length:  30428\n",
      "train_balancing False\n",
      "CV 8\n",
      "\n",
      "\n",
      "******************* M3-RNN *******************\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 150, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 10, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_5 (Masking)             (None, 150, 200)     0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_6 (Masking)             (None, 10, 200)      0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "forwards_1 (LSTM)               (None, 256)          467968      masking_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwords_1 (LSTM)              (None, 256)          467968      masking_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "forwards_2 (LSTM)               (None, 32)           29824       masking_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwards_2 (LSTM)              (None, 32)           29824       masking_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 512)          0           forwards_1[0][0]                 \n",
      "                                                                 backwords_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 64)           0           forwards_2[0][0]                 \n",
      "                                                                 backwards_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_1 (Dropout)            (None, 512)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_2 (Dropout)            (None, 64)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 28)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 604)          0           after_dp_1[0][0]                 \n",
      "                                                                 after_dp_2[0][0]                 \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_1 (Dense)          (None, 380)          229900      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 380)          0           middle_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_2 (Dense)          (None, 450)          171450      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 450)          0           middle_dense_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_3 (Dense)          (None, 200)          90200       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 7)            1407        middle_dense_3[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,488,541\n",
      "Trainable params: 1,488,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61949 samples, validate on 10933 samples\n",
      "Epoch 1/20\n",
      "61949/61949 [==============================] - 1050s 17ms/step - loss: 1.7605 - acc: 0.3120 - val_loss: 1.7162 - val_acc: 0.3307\n",
      "Epoch 2/20\n",
      "61949/61949 [==============================] - 1048s 17ms/step - loss: 1.7105 - acc: 0.3378 - val_loss: 1.7025 - val_acc: 0.3386\n",
      "Epoch 3/20\n",
      "61949/61949 [==============================] - 1053s 17ms/step - loss: 1.6814 - acc: 0.3500 - val_loss: 1.7015 - val_acc: 0.3404\n",
      "Epoch 4/20\n",
      "61949/61949 [==============================] - 1040s 17ms/step - loss: 1.6477 - acc: 0.3629 - val_loss: 1.6866 - val_acc: 0.3455\n",
      "Epoch 5/20\n",
      "61949/61949 [==============================] - 1043s 17ms/step - loss: 1.6083 - acc: 0.3800 - val_loss: 1.6833 - val_acc: 0.3465\n",
      "Epoch 6/20\n",
      "61949/61949 [==============================] - 1042s 17ms/step - loss: 1.5617 - acc: 0.3993 - val_loss: 1.6762 - val_acc: 0.3540\n",
      "Epoch 7/20\n",
      "61949/61949 [==============================] - 1041s 17ms/step - loss: 1.5091 - acc: 0.4197 - val_loss: 1.6821 - val_acc: 0.3521\n",
      " ********* emb7-M3 Learning time: 7319sec ********* \n",
      "Saved model to disk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******************* M3 - Embedding RNN *******************\n",
      "Total test accuracy : 31.25\n",
      "\n",
      "Confusion Matrix\n",
      "[[1570  173   15   54   88   18   64]\n",
      " [ 575  307   30  108  254   28   87]\n",
      " [ 265  195   24  112  211   27   80]\n",
      " [ 247  203   31  132  333   38  121]\n",
      " [ 254  232   41  190  475   76  168]\n",
      " [ 141  137   15  100  264   50  126]\n",
      " [ 167  131   25  139  455   93  220]]\n",
      "\n",
      "f1_score: 31.25\n",
      "precision_score: 31.25\n",
      "recall_score: 31.25\n",
      "\n",
      "31.25 31.25 31.25 31.25\n",
      "\n",
      "\n",
      "Total length:  30428\n",
      "train_balancing False\n",
      "CV 9\n",
      "\n",
      "\n",
      "******************* M3-RNN *******************\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 150, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 10, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_7 (Masking)             (None, 150, 200)     0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_8 (Masking)             (None, 10, 200)      0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "forwards_1 (LSTM)               (None, 256)          467968      masking_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwords_1 (LSTM)              (None, 256)          467968      masking_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "forwards_2 (LSTM)               (None, 32)           29824       masking_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwards_2 (LSTM)              (None, 32)           29824       masking_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           forwards_1[0][0]                 \n",
      "                                                                 backwords_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64)           0           forwards_2[0][0]                 \n",
      "                                                                 backwards_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_1 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_2 (Dropout)            (None, 64)           0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 28)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 604)          0           after_dp_1[0][0]                 \n",
      "                                                                 after_dp_2[0][0]                 \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_1 (Dense)          (None, 380)          229900      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 380)          0           middle_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_2 (Dense)          (None, 450)          171450      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 450)          0           middle_dense_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_3 (Dense)          (None, 200)          90200       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 7)            1407        middle_dense_3[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,488,541\n",
      "Trainable params: 1,488,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 69505 samples, validate on 12266 samples\n",
      "Epoch 1/20\n",
      "69505/69505 [==============================] - 1242s 18ms/step - loss: 1.7565 - acc: 0.3155 - val_loss: 1.7134 - val_acc: 0.3293\n",
      "Epoch 2/20\n",
      "69505/69505 [==============================] - 1175s 17ms/step - loss: 1.7086 - acc: 0.3360 - val_loss: 1.7138 - val_acc: 0.3338\n",
      " ********* emb7-M3 Learning time: 2418sec ********* \n",
      "Saved model to disk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******************* M3 - Embedding RNN *******************\n",
      "Total test accuracy : 32.67\n",
      "\n",
      "Confusion Matrix\n",
      "[[1525  102    0    5  229    7  100]\n",
      " [ 627  141    0   22  373   13  232]\n",
      " [ 239   71    0   21  310   10  203]\n",
      " [ 244   85    1   21  411   29  298]\n",
      " [ 237   85    0   23  606   34  553]\n",
      " [ 138   35    0    9  354   31  399]\n",
      " [ 162   26    1    9  351   42  631]]\n",
      "\n",
      "f1_score: 32.67\n",
      "precision_score: 32.67\n",
      "recall_score: 32.67\n",
      "\n",
      "32.67 32.67 32.67 32.67\n",
      "\n",
      "\n",
      "Total length:  30428\n",
      "train_balancing False\n",
      "CV 10\n",
      "\n",
      "\n",
      "******************* M3-RNN *******************\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 150, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 10, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_9 (Masking)             (None, 150, 200)     0           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_10 (Masking)            (None, 10, 200)      0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "forwards_1 (LSTM)               (None, 256)          467968      masking_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "backwords_1 (LSTM)              (None, 256)          467968      masking_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "forwards_2 (LSTM)               (None, 32)           29824       masking_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "backwards_2 (LSTM)              (None, 32)           29824       masking_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 512)          0           forwards_1[0][0]                 \n",
      "                                                                 backwords_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64)           0           forwards_2[0][0]                 \n",
      "                                                                 backwards_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_1 (Dropout)            (None, 512)          0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "after_dp_2 (Dropout)            (None, 64)           0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 28)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 604)          0           after_dp_1[0][0]                 \n",
      "                                                                 after_dp_2[0][0]                 \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_1 (Dense)          (None, 380)          229900      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 380)          0           middle_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_2 (Dense)          (None, 450)          171450      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 450)          0           middle_dense_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "middle_dense_3 (Dense)          (None, 200)          90200       leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 7)            1407        middle_dense_3[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,488,541\n",
      "Trainable params: 1,488,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77193 samples, validate on 13623 samples\n",
      "Epoch 1/20\n",
      "77193/77193 [==============================] - 1267s 16ms/step - loss: 1.7555 - acc: 0.3150 - val_loss: 1.7157 - val_acc: 0.3313\n",
      "Epoch 2/20\n",
      "77193/77193 [==============================] - 1269s 16ms/step - loss: 1.7091 - acc: 0.3389 - val_loss: 1.7009 - val_acc: 0.3434\n",
      "Epoch 3/20\n",
      "77193/77193 [==============================] - 1270s 16ms/step - loss: 1.6806 - acc: 0.3489 - val_loss: 1.6857 - val_acc: 0.3535\n",
      "Epoch 4/20\n",
      "77193/77193 [==============================] - 1265s 16ms/step - loss: 1.6502 - acc: 0.3636 - val_loss: 1.6842 - val_acc: 0.3479\n",
      "Epoch 5/20\n",
      "77193/77193 [==============================] - 1263s 16ms/step - loss: 1.6149 - acc: 0.3761 - val_loss: 1.6675 - val_acc: 0.3579\n",
      "Epoch 6/20\n",
      "77193/77193 [==============================] - 1269s 16ms/step - loss: 1.5698 - acc: 0.3952 - val_loss: 1.6584 - val_acc: 0.3612\n",
      "Epoch 7/20\n",
      "77193/77193 [==============================] - 1269s 16ms/step - loss: 1.5185 - acc: 0.4158 - val_loss: 1.6667 - val_acc: 0.3630\n",
      " ********* emb7-M3 Learning time: 8873sec ********* \n",
      "Saved model to disk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******************* M3 - Embedding RNN *******************\n",
      "Total test accuracy : 31.94\n",
      "\n",
      "Confusion Matrix\n",
      "[[1596  156    6   15  132   53   54]\n",
      " [ 574  270   15   48  298   97   92]\n",
      " [ 249  157   11   32  273   86   98]\n",
      " [ 213  167   11   37  388  127  147]\n",
      " [ 250  161   15   48  565  209  235]\n",
      " [ 122   89    8   23  406  149  164]\n",
      " [ 151  122    7   44  499  202  302]]\n",
      "\n",
      "f1_score: 31.94\n",
      "precision_score: 31.94\n",
      "recall_score: 31.94\n",
      "\n",
      "31.94 31.94 31.94 31.94\n",
      "\n",
      "\n",
      "\n",
      "****************** 7 M3_runT, average : [4676, 4622, 7319, 2418, 8873] 5581.6 ******************\n"
     ]
    }
   ],
   "source": [
    "auto_balancing=False\n",
    "train_balancing=False\n",
    "data_name = {mozilla:'mozilla', chrome:'chrome', firefox:'firefox', eclipse:'eclipse'}\n",
    "#[0]:workday [1]:openday [2]:activityCnt [3]:cc \n",
    "total_split = {'firefox': [[3,7],[1,2,3,10,30],[3,7,20,50],[1,2,3]], 'chrome':[[3,7],[1,2,3,10,30],[1,2,5,10],[1,2,3]], 'eclipse':[[3,7],[1,2,3,10,30],[1,2,3,5],[1,2,3]]} \n",
    "\n",
    "test_mode = False\n",
    "if test_mode: save_off = True\n",
    "else: save_off = False\n",
    "#save_off=False\n",
    "\n",
    "emb_load = False\n",
    "\n",
    "max_activity_len = 10\n",
    "min_activity_len = 1 # workday 1-10\n",
    "\n",
    "\n",
    "task_same = False\n",
    "\n",
    "fixtime_thr = [0,1,2,4,10,20]\n",
    "label_num=len(fixtime_thr)+1\n",
    "lambda_list = [label_num]\n",
    "#\n",
    "for lamb in lambda_list:\n",
    "    #if lamb==2 or lamb==4: continue\n",
    "    if task_same:\n",
    "        emb_std = lamb\n",
    "        seq_std_list = [lamb]\n",
    "        mode_list = [0]# 0,1 모두 돈다\n",
    "    else:\n",
    "        emb_std = lamb\n",
    "        seq_std_list = [2]\n",
    "        #seq_std_list = list(lambda_list)\n",
    "        #seq_std_list.remove(emb_std)\n",
    "        mode_list = [0] # 0,1 모두 돈다\n",
    "    #\n",
    "    m3_runT = []\n",
    "    for mode in mode_list: # 0:3-5, 1:4-6\n",
    "        if mode==1: continue\n",
    "        if mode==0:\n",
    "            doc2vec_rnn = True\n",
    "            m3_stacking = False\n",
    "            doc2vec_avg = False\n",
    "            multi_layer_LSTM = False\n",
    "        elif mode ==1:\n",
    "            doc2vec_rnn = False \n",
    "            m3_stacking = False\n",
    "            doc2vec_avg = True\n",
    "            multi_layer_LSTM = False\n",
    "        for i in range(6,11):\n",
    "            totalLength = len(all_data)\n",
    "            print('Total length: ', totalLength)\n",
    "            splitLength = int(totalLength / (numCV + 1))\n",
    "            print('train_balancing', train_balancing)\n",
    "            # Split cross validation set\n",
    "            print ('CV',i)\n",
    "            if test_mode:\n",
    "                train_data = all_data[:30]#i*splitLength]\n",
    "                train_history = all_history[:30]#i*splitLength]\n",
    "                train_3to1 = sum([len(r) for r in train_data])       \n",
    "                train_time = all_time[:train_3to1]#i*splitLength]\n",
    "                train_workday = all_workday[:train_3to1]\n",
    "                train_recentday = all_recentday[:train_3to1]\n",
    "                train_openday = all_openday[:train_3to1]\n",
    "                train_activitycnt = all_activitycnt[:train_3to1]\n",
    "                train_cc = all_cc[:train_3to1]\n",
    "                train_writer = all_writer[:train_3to1]\n",
    "                train_bugid = all_bugid[:train_3to1]\n",
    "            else:\n",
    "                train_data = all_data[:i*splitLength]\n",
    "                train_history = all_history[:i*splitLength]\n",
    "                train_3to1 = sum([len(r) for r in train_data])       \n",
    "                train_time = all_time[:train_3to1]#i*splitLength]\n",
    "                train_workday = all_workday[:train_3to1]\n",
    "                train_recentday = all_recentday[:train_3to1]\n",
    "                train_openday = all_openday[:train_3to1]\n",
    "                train_activitycnt = all_activitycnt[:train_3to1]\n",
    "                train_cc = all_cc[:train_3to1]\n",
    "                train_writer = all_writer[:train_3to1]\n",
    "                train_bugid = all_bugid[:train_3to1]\n",
    "\n",
    "            # ===================================================================== \n",
    "            updated_train_data = []    \n",
    "            updated_train_history = []    \n",
    "            updated_train_time = []\n",
    "            #updated_train_stream = []\n",
    "            #updated_train_workday = []\n",
    "            #updated_train_bugid = []\n",
    "\n",
    "            j=0\n",
    "            for bug1, bug2 in zip(train_data, train_history):\n",
    "                train_data_list = []\n",
    "                train_history_list = []\n",
    "                if len(bug1)>=min_activity_len:\n",
    "                    for act1, act2 in zip(bug1, bug2):\n",
    "                        current_train_filter = [word for word in act1 if word in vocabulary]\n",
    "                        his_current_train_filter = [word for word in act2 if word in vocabulary]\n",
    "                        train_data_list.append(current_train_filter)\n",
    "                        train_history_list.append(his_current_train_filter)\n",
    "                        #updated_train_time.append(train_time[j])\n",
    "                        #updated_train_stream.append(train_stream[j])\n",
    "                        #updated_train_workday.append(train_workday[j])\n",
    "                        #updated_train_bugid.append(train_bugid[j])\n",
    "                        j+=1\n",
    "                    updated_train_data.append(train_data_list)\n",
    "                    updated_train_history.append(train_history_list)\n",
    "                else:\n",
    "                    j+=len(bug1)\n",
    "\n",
    "            del train_data, train_history, #train_bugid, train_workday\n",
    "            gc.collect()\n",
    "\n",
    "            # ===================================================================== \n",
    "            updated_train_time = [0 if x<=fixtime_thr[0] else 1 if x<=fixtime_thr[1] else 2 if x<=fixtime_thr[2] else 3 if x<=fixtime_thr[3] else 4 if x<=fixtime_thr[4] else 5 if x<=fixtime_thr[5] else 6 for x in train_time]\n",
    "            curr_split = total_split[data_name[path]][0]\n",
    "            updated_train_workday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 for x in train_workday]\n",
    "            updated_train_recentday = []\n",
    "            for x in train_recentday:\n",
    "                tmp = 0 \n",
    "                for j,y in enumerate(x):\n",
    "                    tmp+= pow(2,j)*y\n",
    "                updated_train_recentday.append(tmp) \n",
    "            curr_split = total_split[data_name[path]][1]\n",
    "            updated_train_openday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in train_openday]\n",
    "            curr_split = total_split[data_name[path]][2]\n",
    "            updated_train_activitycnt = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in train_activitycnt]\n",
    "            curr_split = total_split[data_name[path]][3]\n",
    "            updated_train_cc = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 for x in train_cc]\n",
    "            updated_train_writer = train_writer\n",
    "            updated_train_bugid = train_bugid\n",
    "            updated_train_data_1d = [y for x in updated_train_data for y in x]\n",
    "            updated_train_history_1d = [y for x in updated_train_history for y in x]\n",
    "            train_col_size = [len(x) for x in updated_train_data]\n",
    "            train_n = len(updated_train_data_1d)\n",
    "            train_df =pd.DataFrame({'x1': updated_train_data_1d, 'x2': updated_train_history_1d, \n",
    "                                    's1': updated_train_workday, 's2':updated_train_recentday, 's3':updated_train_activitycnt,\n",
    "                                    's4':updated_train_cc, 's5':updated_train_writer, 's6':updated_train_openday,\n",
    "                                    'bugid':updated_train_bugid,\n",
    "                                    'y1': updated_train_time})\n",
    "            #print(train_df[:10])\n",
    "            del updated_train_time, updated_train_history, updated_train_data, updated_train_data_1d, updated_train_history_1d\n",
    "            del updated_train_workday, updated_train_recentday, updated_train_activitycnt, updated_train_cc, updated_train_writer, updated_train_bugid\n",
    "            gc.collect()\n",
    "\n",
    "            # =============================\n",
    "            # BALANCE RESAMPLING\n",
    "            if train_balancing:\n",
    "                class_count = train_df.y2.value_counts()\n",
    "                print('\\nClass 0:', class_count[0])\n",
    "                print('Class 1:', class_count[1])\n",
    "                df_class_0 = train_df[train_df['y2']==0]\n",
    "                df_class_1 = train_df[train_df['y2']==1]\n",
    "                if class_count[0]<class_count[1]:\n",
    "                    under_class_cnt = class_count[0]\n",
    "                    df_class_1 = df_class_1.sample(under_class_cnt)\n",
    "                else:\n",
    "                    under_class_cnt = class_count[1]\n",
    "                    df_class_0 = df_class_0.sample(under_class_cnt)\n",
    "                train_df = pd.concat([df_class_0, df_class_1], axis=0)\n",
    "                train_df = train_df.sort_index()\n",
    "                train_n = len(train_df)\n",
    "                print(train_df[:10],'\\n')\n",
    "                print('\\nRandom under-sampling:')\n",
    "                print(train_df.y2.value_counts(),'\\n')\n",
    "                # new train_col_size\n",
    "                cnt=0\n",
    "                train_col_size = []\n",
    "                prev_id = train_df.iloc[0]['bugid']\n",
    "                for x in train_df.bugid.values:\n",
    "                    if prev_id != x:\n",
    "                        train_col_size.append(cnt)\n",
    "                        cnt = 0\n",
    "                    prev_id = x\n",
    "                    cnt+=1\n",
    "                train_col_size.append(cnt)\n",
    "\n",
    "            # ===================================================================== \n",
    "            X_train_last = np.zeros(shape=[train_n, max_sentence_len, embed_size_word2vec], dtype='float32') # len(updated_train_data) # train_len*2\n",
    "            X_train_history = np.zeros(shape=[train_n, max_his_len, embed_size_word2vec], dtype='float32') # len(updated_train_history)\n",
    "            Y_train = np.empty(shape=[train_n,1], dtype='int32') # len(updated_train_time)\n",
    "\n",
    "            j=0\n",
    "            for curr_row1, curr_row2 in zip(train_df.x1.values, train_df.x2.values):\n",
    "                if len(curr_row1)>max_sentence_len:\n",
    "                    start_loc = len(curr_row1) - max_sentence_len\n",
    "                else: \n",
    "                    start_loc = 0\n",
    "                sequence_cnt = 0    \n",
    "                for item1 in curr_row1[start_loc:]:\n",
    "                    if item1 in vocabulary:\n",
    "                        X_train_last[j, sequence_cnt, :] = wordvec_model[item1] \n",
    "                        sequence_cnt = sequence_cnt + 1                \n",
    "                        if sequence_cnt == max_sentence_len-1:\n",
    "                            break  \n",
    "                for k in range(sequence_cnt, max_sentence_len):\n",
    "                    X_train_last[j, k, :] = np.zeros((1,embed_size_word2vec))   \n",
    "                if len(curr_row2)>max_his_len:\n",
    "                    start_loc = len(curr_row2) - max_his_len\n",
    "                else: \n",
    "                    start_loc = 0\n",
    "                sequence_cnt = 0\n",
    "                for item2 in curr_row2[start_loc:]:\n",
    "                    if item2 in vocabulary:\n",
    "                        X_train_history[j, sequence_cnt, :] = wordvec_model[item2] \n",
    "                        sequence_cnt = sequence_cnt + 1                \n",
    "                        if sequence_cnt == max_his_len-1:\n",
    "                                break \n",
    "                for k in range(sequence_cnt, max_his_len):\n",
    "                    X_train_history[j, k, :] = np.zeros((1,embed_size_word2vec))\n",
    "                Y_train[j] = train_df.y1.values[j]\n",
    "                j+=1\n",
    "            y_train = np_utils.to_categorical(Y_train, label_num)\n",
    "\n",
    "            if doc2vec_avg:\n",
    "                X_train_doc = np.zeros(shape=[train_n, embed_size_word2vec], dtype='float32')\n",
    "                X_train_his_doc = np.zeros(shape=[train_n, embed_size_word2vec], dtype='float32')\n",
    "                for j,words in enumerate(X_train_last):\n",
    "                    cnt=0\n",
    "                    smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                    for word in words:\n",
    "                        if not np.any(word): # All-zero element?\n",
    "                            break\n",
    "                        smt+=word\n",
    "                        cnt+=1\n",
    "                    if not np.any(smt):\n",
    "                        avg = smt\n",
    "                    else:\n",
    "                        avg = smt/cnt\n",
    "                    X_train_doc[j] = avg\n",
    "                for j,words in enumerate(X_train_history):\n",
    "                    cnt=0\n",
    "                    smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                    for word in words:\n",
    "                        if not np.any(word): # All-zero element?\n",
    "                            break\n",
    "                        smt+=word\n",
    "                        cnt+=1\n",
    "                    if not np.any(smt):\n",
    "                        avg = smt\n",
    "                    else:\n",
    "                        avg = smt/cnt\n",
    "                    X_train_his_doc[j] = avg\n",
    "\n",
    "                X_train_avg = np.concatenate((X_train_doc, X_train_his_doc),axis=1)\n",
    "                del X_train_last, X_train_history\n",
    "                gc.collect()\n",
    "\n",
    "            # Statistic\n",
    "            X_train_workday = np.asarray(train_df.s1.values, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][0])+1\n",
    "            X_train_workday = np_utils.to_categorical(X_train_workday, class_num)\n",
    "            X_train_openday = np.asarray(train_df.s6.values, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][1])+1\n",
    "            X_train_openday = np_utils.to_categorical(X_train_openday, class_num)\n",
    "            X_train_recentday = np.asarray(train_df.s2.values, dtype=np.float32)\n",
    "            class_num = 8\n",
    "            X_train_recentday = np_utils.to_categorical(X_train_recentday, class_num)\n",
    "            X_train_activitycnt = np.asarray(train_df.s3.values, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][2])+1\n",
    "            X_train_activitycnt = np_utils.to_categorical(X_train_activitycnt, class_num)\n",
    "            X_train_cc = np.asarray(train_df.s4.values, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][3])+1\n",
    "            X_train_cc = np_utils.to_categorical(X_train_cc, class_num)\n",
    "            X_train_writer = np.array([np.array(x, dtype=np.float32) for x in train_df.s5.values])\n",
    "\n",
    "            X_train_stat = np.concatenate((X_train_workday, X_train_recentday),axis=1)\n",
    "            X_train_stat = np.concatenate((X_train_stat, X_train_openday),axis=1)\n",
    "            X_train_stat = np.concatenate((X_train_stat, X_train_activitycnt),axis=1)\n",
    "            X_train_stat = np.concatenate((X_train_stat, X_train_cc),axis=1)\n",
    "            X_train_stat = np.concatenate((X_train_stat, X_train_writer),axis=1)\n",
    "\n",
    "            # Model\n",
    "            if doc2vec_rnn:\n",
    "                h1=256\n",
    "                h2 = 32\n",
    "                last_input = Input(shape=(max_sentence_len, embed_size_word2vec), dtype='float32')\n",
    "                mask_1 = Masking(mask_value=0)(last_input)\n",
    "                forwards_1 = LSTM(h1, name='forwards_1')(mask_1)\n",
    "                backwards_1 = LSTM(h1, go_backwards=True, name='backwords_1')(mask_1)         \n",
    "                merged_1 = keras.layers.concatenate([forwards_1, backwards_1], axis=-1)\n",
    "                after_dp_1= Dropout(0.5, name='after_dp_1')(merged_1)\n",
    "                tmp_output_1 = Dense(label_num, activation='softmax', name='tmp_output_1')(after_dp_1)\n",
    "\n",
    "                history_input = Input(shape=(max_his_len, embed_size_word2vec), dtype='float32')\n",
    "                mask_2 = Masking(mask_value=0)(history_input)\n",
    "                forwards_2 = LSTM(h2, name='forwards_2')(mask_2)\n",
    "                backwards_2 = LSTM(h2, go_backwards=True, name='backwards_2')(mask_2)\n",
    "                merged_2 = keras.layers.concatenate([forwards_2, backwards_2], axis=-1)\n",
    "                after_dp_2 = Dropout(0.5, name='after_dp_2')(merged_2)\n",
    "                tmp_output_2 = Dense(label_num, activation='softmax', name='tmp_output_2')(after_dp_2)\n",
    "\n",
    "                # stat\n",
    "                stat_input = Input(shape=(len(X_train_stat[0]),), dtype='float32')\n",
    "\n",
    "                middle_merge = keras.layers.concatenate([after_dp_1, after_dp_2, stat_input], axis=-1)\n",
    "                middle_dense = Dense(380, name='middle_dense_1')(middle_merge) \n",
    "                middle_dense = LeakyReLU(alpha=0.3)(middle_dense)\n",
    "                middle_dense = Dense(450, name='middle_dense_2')(middle_dense) \n",
    "                middle_dense = LeakyReLU(alpha=0.2)(middle_dense)\n",
    "                middle_dense = Dense(200, name='middle_dense_3', activation='tanh')(middle_dense) \n",
    "                #middle_dense = LeakyReLU(alpha=0.5)(middle_dense)\n",
    "                #last_dense = Dense(100, name='middle_dense_4')(middle_dense) \n",
    "                #last_dense = LeakyReLU(alpha=0.1)(last_dense)\n",
    "                output = Dense(label_num, activation='softmax', name='output')(middle_dense)\n",
    "                if m3_stacking:\n",
    "                    user_model = Model(input=last_input, output = tmp_output_1)\n",
    "                    sys_model = Model(input=history_input, output=tmp_output_2)\n",
    "                embedding_size = 200\n",
    "                m3 = Model(input=[last_input, history_input, stat_input], output=output)    \n",
    "\n",
    "            elif doc2vec_avg:\n",
    "                docAvg_input = Input(shape=(len(X_train_avg[0]),), dtype='float32')\n",
    "                stat_input = Input(shape=(len(X_train_stat[0]),), dtype='float32')\n",
    "                concat_input = keras.layers.concatenate([docAvg_input, stat_input], axis=-1)\n",
    "                middle_dense = Dense(230, name='middle_dense_1')(concat_input) \n",
    "                middle_dense = LeakyReLU(alpha=0.3)(middle_dense)\n",
    "                middle_dense = Dense(300, name='middle_dense_2')(middle_dense) \n",
    "                middle_dense = LeakyReLU(alpha=0.2)(middle_dense)\n",
    "                middle_dense = Dense(180, name='middle_dense_3')(middle_dense) \n",
    "                middle_dense = LeakyReLU(alpha=0.5)(middle_dense)\n",
    "                last_dense = Dense(100, name='middle_dense_4')(middle_dense) \n",
    "                last_dense = LeakyReLU(alpha=0.1)(last_dense)\n",
    "                output = Dense(label_num, activation='softmax', name='output')(last_dense)\n",
    "\n",
    "                embedding_size = 180\n",
    "                m3 = Model(input=[docAvg_input, stat_input], output=output)    \n",
    "\n",
    "            adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "            m3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "            \n",
    "            # TRAIN MODEL\n",
    "            es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "            if doc2vec_rnn:\n",
    "                if m3_stacking:\n",
    "                    # Except for X_train_last == []\n",
    "                    mask = [0 for x in range(embed_size_word2vec)]\n",
    "                    condition_bool =  np.ndarray.all((X_train_last==mask), axis=2).all(axis=1)\n",
    "                    X_train_last_mask = np.delete(X_train_last, np.where(condition_bool), 0)\n",
    "                    y_train_mask = np.delete(y_train, np.where(np.where(condition_bool)), 0)\n",
    "                    X_train_last_mask, X_valid_last_mask, y_train_mask, y_valid_mask = train_test_split(X_train_last_mask, y_train_mask, test_size=0.15, shuffle= True)\n",
    "\n",
    "                    print('\\n******************* User-RNN *******************')\n",
    "                    user_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "                    user_model.summary()\n",
    "                    hist = user_model.fit(X_train_last_mask, y_train_mask, batch_size=batch_size, \n",
    "                                 validation_data = (X_valid_last_mask, y_valid_mask), epochs=20, callbacks=[es])\n",
    "                    print(hist.history)\n",
    "\n",
    "                    # Except for X_train_history == []\n",
    "                    condition_bool =  np.ndarray.all((X_train_history==mask), axis=2).all(axis=1)\n",
    "                    X_train_his_mask = np.delete(X_train_history, np.where(condition_bool), 0)\n",
    "                    y_train_mask = np.delete(y_train, np.where(condition_bool), 0)\n",
    "                    X_train_his_mask, X_valid_his_mask, y_train_mask, y_valid_mask = train_test_split(X_train_his_mask, y_train_mask, test_size=0.15, shuffle= True)\n",
    "\n",
    "                    print('\\n\\n******************* Sys-RNN *******************')\n",
    "                    sys_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "                    sys_model.summary()\n",
    "                    hist = sys_model.fit(X_train_his_mask, y_train_mask, batch_size=batch_size, \n",
    "                                 validation_data = (X_valid_his_mask, y_valid_mask), epochs=20, callbacks=[es])\n",
    "                    print(hist.history)\n",
    "\n",
    "                    # Save sys_log weights\n",
    "                    if not save_off:\n",
    "                        model_json = user_model.to_json()\n",
    "                        model_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.json\"\n",
    "                        weight_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                        with open(model_name,\"w\") as json_file :\n",
    "                            json_file.write(model_json)\n",
    "                            user_model.save_weights(weight_name)\n",
    "                        print(\"Saved model to disk\\n\\n\\n\")\n",
    "\n",
    "                        model_json = sys_model.to_json()\n",
    "                        model_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.json\"\n",
    "                        weight_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                        with open(model_name,\"w\") as json_file :\n",
    "                            json_file.write(model_json)\n",
    "                            sys_model.save_weights(weight_name)\n",
    "                        print(\"Saved model to disk\\n\\n\\n\")\n",
    "                        del user_model, sys_model\n",
    "\n",
    "                    # Load weights (user_comment + sys_log)\n",
    "                    weight_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                    m3.load_weights(weight_name, by_name=True)\n",
    "                    weight_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                    m3.load_weights(weight_name, by_name=True)\n",
    "\n",
    "                print('\\n\\n******************* M3-RNN *******************')  \n",
    "                if emb_load:\n",
    "                    pass\n",
    "                else:\n",
    "                    X_train_last_sp, X_valid_last, X_train_history_sp, X_valid_history, X_train_stat_sp, X_valid_stat, y_train_sp, y_valid = train_test_split(X_train_last, X_train_history, X_train_stat, y_train, test_size=0.15, shuffle= True)\n",
    "                    m3.summary()\n",
    "                    start = time.time()\n",
    "                    hist = m3.fit([X_train_last_sp, X_train_history_sp, X_train_stat_sp], y_train_sp, batch_size=batch_size, \n",
    "                                     validation_data = ([X_valid_last, X_valid_history, X_valid_stat], y_valid), epochs=20, callbacks=[es])\n",
    "                    end = time.time()\n",
    "                    m3_runT.append(int(end-start))\n",
    "                    print(\" ********* emb%d-M3 Learning time: %dsec ********* \" %(emb_std,int(end-start)))\n",
    "                    del X_train_last_sp, X_train_history_sp, X_train_stat_sp, y_train_sp\n",
    "                    del X_valid_last, X_valid_history, X_valid_stat, y_valid\n",
    "                    gc.collect()\n",
    "                    if not save_off:\n",
    "                        # Save model\n",
    "                        model_json = m3.to_json()\n",
    "                        model_name = data_name[path]+\"_M3_tanh_limit100_class\"+str(label_num)+\"_cv\"+str(i)+\".json\"\n",
    "                        weight_name = data_name[path]+\"_M3_tanh_limit100_class\"+str(label_num)+\"_cv\"+str(i)+\".h5\"\n",
    "                        with open(model_name,\"w\") as json_file :\n",
    "                            json_file.write(model_json)\n",
    "                            m3.save_weights(weight_name)\n",
    "                        print(\"Saved model to disk\\n\\n\\n\")\n",
    "\n",
    "            #========================================================================================\n",
    "            # TEST DATA\n",
    "            #========================================================================================\n",
    "            if test_mode:\n",
    "                test_data = all_data[30:50]#i*splitLength:(i+1)*splitLength] \n",
    "                test_history = all_history[30:50]#i*splitLength:(i+1)*splitLength]\n",
    "                test_3to1 = sum([len(r) for r in test_data])\n",
    "                test_time = all_time[train_3to1:train_3to1+test_3to1]#i*splitLength:(i+1)*splitLength]\n",
    "                #test_stream = all_stream[train_3to1 : train_3to1+test_3to1]\n",
    "                test_workday = all_workday[train_3to1:train_3to1+test_3to1]\n",
    "                test_recentday = all_recentday[train_3to1:train_3to1+test_3to1]\n",
    "                test_openday = all_openday[train_3to1:train_3to1+test_3to1]\n",
    "                test_activitycnt = all_activitycnt[train_3to1:train_3to1+test_3to1]\n",
    "                test_cc = all_cc[train_3to1:train_3to1+test_3to1]\n",
    "                test_writer = all_writer[train_3to1:train_3to1+test_3to1]\n",
    "            else:\n",
    "                test_data = all_data[i*splitLength:(i+1)*splitLength]\n",
    "                test_history = all_history[i*splitLength:(i+1)*splitLength]\n",
    "                test_3to1 = sum([len(r) for r in test_data])       \n",
    "                test_time = all_time[train_3to1 : train_3to1+test_3to1]#i*splitLength]\n",
    "                #test_stream = all_stream[train_3to1:train_3to1+test_3to1]\n",
    "                test_workday = all_workday[train_3to1:train_3to1+test_3to1]\n",
    "                test_recentday = all_recentday[train_3to1:train_3to1+test_3to1]\n",
    "                test_openday = all_openday[train_3to1:train_3to1+test_3to1]\n",
    "                test_activitycnt = all_activitycnt[train_3to1:train_3to1+test_3to1]\n",
    "                test_cc = all_cc[train_3to1:train_3to1+test_3to1]\n",
    "                test_writer = all_writer[train_3to1:train_3to1+test_3to1]\n",
    "\n",
    "            # ===================================================================== \n",
    "            updated_test_data = []    \n",
    "            updated_test_history = []    \n",
    "            updated_test_time = []\n",
    "            #updated_test_stream = []\n",
    "            #updated_test_workday = []\n",
    "            #updated_test_bugid = []\n",
    "\n",
    "            j=0\n",
    "            for bug1, bug2 in zip(test_data, test_history):\n",
    "                test_data_list = []\n",
    "                test_history_list = []\n",
    "                if len(bug1)>=min_activity_len:\n",
    "                    for act1, act2 in zip(bug1, bug2):\n",
    "                        current_test_filter = [word for word in act1 if word in vocabulary]\n",
    "                        his_current_test_filter = [word for word in act2 if word in vocabulary]\n",
    "                        test_data_list.append(current_test_filter)\n",
    "                        test_history_list.append(his_current_test_filter)\n",
    "                        updated_test_time.append(test_time[j])\n",
    "                        #updated_test_stream.append(test_stream[j])\n",
    "                        #updated_test_workday.append(test_workday[j])\n",
    "                        #updated_test_bugid.append(test_bugid[j])\n",
    "                        j+=1\n",
    "                    updated_test_data.append(test_data_list)\n",
    "                    updated_test_history.append(test_history_list)\n",
    "                else:\n",
    "                    j+=len(bug1)\n",
    "\n",
    "            del test_data, test_history, #test_bugid, test_workday\n",
    "            gc.collect()\n",
    "\n",
    "            # ===================================================================== \n",
    "            updated_test_time = [0 if x<=fixtime_thr[0] else 1 if x<=fixtime_thr[1] else 2 if x<=fixtime_thr[2] else 3 if x<=fixtime_thr[3] else 4 if x<=fixtime_thr[4] else 5 if x<=fixtime_thr[5] else 6 for x in test_time]\n",
    "            curr_split = total_split[data_name[path]][0]\n",
    "            updated_test_workday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 for x in test_workday]\n",
    "            updated_test_recentday = []\n",
    "            for x in test_recentday:\n",
    "                tmp = 0 \n",
    "                for j,y in enumerate(x):\n",
    "                    tmp+= pow(2,j)*y\n",
    "                updated_test_recentday.append(tmp) \n",
    "            curr_split = total_split[data_name[path]][1]\n",
    "            updated_test_openday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in test_openday]\n",
    "            curr_split = total_split[data_name[path]][2]\n",
    "            updated_test_activitycnt = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in test_activitycnt]\n",
    "            curr_split = total_split[data_name[path]][3]\n",
    "            updated_test_cc = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 for x in test_cc]\n",
    "            updated_test_writer = test_writer\n",
    "\n",
    "            updated_test_data_1d = [y for x in updated_test_data for y in x]\n",
    "            updated_test_history_1d = [y for x in updated_test_history for y in x]\n",
    "            test_col_size = [len(x) for x in updated_test_data]\n",
    "            test_n = len(updated_test_data_1d)\n",
    "            del updated_test_data, updated_test_history, #test_workday, test_recentday,test_activitycnt\n",
    "            gc.collect()\n",
    "\n",
    "            # ===================================================================== \n",
    "            X_test_last = np.zeros(shape=[test_n, max_sentence_len, embed_size_word2vec], dtype='float32') # len(updated_test_data) # test_len*2\n",
    "            X_test_history = np.zeros(shape=[test_n, max_his_len, embed_size_word2vec], dtype='float32') # len(updated_test_history)\n",
    "            #X_test_stream = np.empty(shape=[test_n, limit_day], dtype='float32')\n",
    "            #X_test_workday = np.empty(shape=[test_n, max_workday], dtype='float32')\n",
    "            Y_test = np.empty(shape=[test_n,1], dtype='int32') # len(updated_test_time)\n",
    "\n",
    "            j=0\n",
    "            for curr_row1, curr_row2 in zip(updated_test_data_1d, updated_test_history_1d):\n",
    "                if len(curr_row1)>max_sentence_len:\n",
    "                    start_loc = len(curr_row1) - max_sentence_len\n",
    "                else: \n",
    "                    start_loc = 0\n",
    "                sequence_cnt = 0    \n",
    "                for item1 in curr_row1[start_loc:]:\n",
    "                    if item1 in vocabulary:\n",
    "                        X_test_last[j, sequence_cnt, :] = wordvec_model[item1] \n",
    "                        sequence_cnt = sequence_cnt + 1                \n",
    "                        if sequence_cnt == max_sentence_len-1:\n",
    "                            break  \n",
    "                for k in range(sequence_cnt, max_sentence_len):\n",
    "                    X_test_last[j, k, :] = np.zeros((1,embed_size_word2vec))   \n",
    "                if len(curr_row2)>max_his_len:\n",
    "                    start_loc = len(curr_row2) - max_his_len\n",
    "                else: \n",
    "                    start_loc = 0\n",
    "                sequence_cnt = 0\n",
    "                for item2 in curr_row2[start_loc:]:\n",
    "                    if item2 in vocabulary:\n",
    "                        X_test_history[j, sequence_cnt, :] = wordvec_model[item2] \n",
    "                        sequence_cnt = sequence_cnt + 1                \n",
    "                        if sequence_cnt == max_his_len-1:\n",
    "                                break \n",
    "                for k in range(sequence_cnt, max_his_len):\n",
    "                    X_test_history[j, k, :] = np.zeros((1,embed_size_word2vec))\n",
    "                Y_test[j] = updated_test_time[j]\n",
    "                j+=1\n",
    "\n",
    "            if doc2vec_avg:\n",
    "                X_test_doc = np.zeros(shape=[test_n, embed_size_word2vec], dtype='float32')\n",
    "                X_test_his_doc = np.zeros(shape=[test_n, embed_size_word2vec], dtype='float32')\n",
    "                for j,words in enumerate(X_test_last):\n",
    "                    cnt=0\n",
    "                    smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                    for word in words:\n",
    "                        if not np.any(word): # All-zero element?\n",
    "                            break\n",
    "                        smt+=word\n",
    "                        cnt+=1\n",
    "                    if not np.any(smt):\n",
    "                        avg = smt\n",
    "                    else:\n",
    "                        avg = smt/cnt\n",
    "                    X_test_doc[j] = avg\n",
    "                for j,words in enumerate(X_test_history):\n",
    "                    cnt=0\n",
    "                    smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                    for word in words:\n",
    "                        if not np.any(word): # All-zero element?\n",
    "                            break\n",
    "                        smt+=word\n",
    "                        cnt+=1\n",
    "                    if not np.any(smt):\n",
    "                        avg = smt\n",
    "                    else:\n",
    "                        avg = smt/cnt\n",
    "                    X_test_his_doc[j] = avg\n",
    "\n",
    "                X_test_avg = np.concatenate((X_test_doc, X_test_his_doc),axis=1)\n",
    "                del X_test_last, X_test_history\n",
    "                gc.collect()\n",
    "\n",
    "            # Statistic\n",
    "            X_test_workday = np.asarray(updated_test_workday, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][0])+1\n",
    "            X_test_workday = np_utils.to_categorical(X_test_workday, class_num)\n",
    "            X_test_openday = np.asarray(updated_test_openday, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][1])+1\n",
    "            X_test_openday = np_utils.to_categorical(X_test_openday, class_num)\n",
    "            X_test_recentday = np.asarray(updated_test_recentday, dtype=np.float32)\n",
    "            class_num = 8\n",
    "            X_test_recentday = np_utils.to_categorical(X_test_recentday, class_num)\n",
    "            X_test_activitycnt = np.asarray(updated_test_activitycnt, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][2])+1\n",
    "            X_test_activitycnt = np_utils.to_categorical(X_test_activitycnt, class_num)\n",
    "            X_test_cc = np.asarray(updated_test_cc, dtype=np.float32)\n",
    "            class_num = len(total_split[data_name[path]][3])+1\n",
    "            X_test_cc = np_utils.to_categorical(X_test_cc, class_num)\n",
    "            X_test_writer = np.asarray(updated_test_writer, dtype=np.float32)\n",
    "\n",
    "            X_test_stat = np.concatenate((X_test_workday, X_test_recentday),axis=1)\n",
    "            X_test_stat = np.concatenate((X_test_stat, X_test_openday),axis=1)\n",
    "            X_test_stat = np.concatenate((X_test_stat, X_test_activitycnt),axis=1)\n",
    "            X_test_stat = np.concatenate((X_test_stat, X_test_cc),axis=1)\n",
    "            X_test_stat = np.concatenate((X_test_stat, X_test_writer),axis=1)\n",
    "\n",
    "\n",
    "            #========================================================================================\n",
    "            # M3 PREDICT & ACCURACY \n",
    "            #========================================================================================\n",
    "            if mode==0:\n",
    "                print('\\n\\n******************* M3 - Embedding RNN *******************') \n",
    "            elif mode==1:\n",
    "                print('\\n\\n******************* M4 - Embedding RNN *******************') \n",
    "            if doc2vec_rnn:\n",
    "                predict = m3.predict([X_test_last, X_test_history, X_test_stat])  \n",
    "                del X_test_last, X_test_history, X_test_stat\n",
    "            elif doc2vec_avg:\n",
    "                predict = m3.predict([X_test_avg, X_test_stat]) \n",
    "                del X_test_avg, X_test_stat\n",
    "            predictY = np.argmax(predict, axis=1)\n",
    "            corrects = np.nonzero(predictY.reshape((-1,1)) == Y_test) #.reshape((-1,))\n",
    "            accu = len(corrects[0])/len(Y_test)\n",
    "            print(\"Total test accuracy : %.2f\" % (accu*100))\n",
    "            print('\\nConfusion Matrix')\n",
    "            cm = confusion_matrix(Y_test, predictY)\n",
    "            print(cm)\n",
    "            total = sum(cm[0])+sum(cm[1])\n",
    "            #print('Predict 0: %.2f' % ((cm[0,0]+cm[1,0])/total*100))\n",
    "            #print('Predict 1: %.2f' % ((cm[0,1]+cm[1,1])/total*100))\n",
    "            f1 = (f1_score(Y_test, predictY, average=\"micro\"))\n",
    "            prec= (precision_score(Y_test, predictY, average=\"micro\"))\n",
    "            recall = (recall_score(Y_test, predictY, average=\"micro\"))\n",
    "            print('\\nf1_score: %.2f' % (f1*100))\n",
    "            print('precision_score: %.2f'% (prec*100)) # class 0 accuracy\n",
    "            print('recall_score: %.2f' % (recall*100)) # class 1 accuracy\n",
    "            print('\\n%.2f %.2f %.2f %.2f\\n' %(prec*100, recall*100, f1*100, accu*100))\n",
    "            print()\n",
    "            del m3\n",
    "            gc.collect()\n",
    "        print('\\n******************',emb_std, 'M3_runT, average :',m3_runT, sum(m3_runT)/len(m3_runT),'******************')\n",
    "        #print('******************',seq_std, 'M5_runT, average :', m5_runT, sum(m5_runT)/len(m5_runT),'******************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequence-embedding Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_balancing=False\n",
    "train_balancing=False\n",
    "data_name = {mozilla:'mozilla', chrome:'chrome', firefox:'firefox', eclipse:'eclipse'}\n",
    "#[0]:workday [1]:openday [2]:activityCnt [3]:cc \n",
    "total_split = {'firefox': [[3,7],[1,2,3,10,30],[3,7,20,50],[1,2,3]], 'chrome':[[3,7],[1,2,3,10,30],[1,2,5,10],[1,2,3]], 'eclipse':[[3,7],[1,2,3,10,30],[1,2,3,5],[1,2,3]]} \n",
    "\n",
    "test_mode = False\n",
    "if test_mode: save_off = True\n",
    "else: save_off = False\n",
    "#save_off=False\n",
    "\n",
    "emb_load = True\n",
    "\n",
    "max_activity_len = 10\n",
    "min_activity_len = 1 # workday 1-10\n",
    "\n",
    "\n",
    "task_same = True\n",
    "\n",
    "#Task Type\n",
    "ResLSTM = True\n",
    "\n",
    "fixtime_thr = [0,1,2,4,10,20]\n",
    "label_num=len(fixtime_thr)+1\n",
    "lambda_list = [label_num]\n",
    "#\n",
    "for lamb in lambda_list:\n",
    "    #if lamb==2 or lamb==4: continue\n",
    "    if task_same:\n",
    "        emb_std = lamb\n",
    "        seq_std_list = [lamb]\n",
    "        mode_list = [0]# 0,1 모두 돈다\n",
    "    else:\n",
    "        emb_std = lamb\n",
    "        seq_std_list = [100]\n",
    "        #seq_std_list = list(lambda_list)\n",
    "        #seq_std_list.remove(emb_std)\n",
    "        mode_list = [0] # 0,1 모두 돈다\n",
    "    #\n",
    "    for seq_std in seq_std_list:\n",
    "        all_class = [0 if x<=seq_std else 1 for x in all_time]\n",
    "        c0 = all_class.count(0)\n",
    "        c1 = all_class.count(1)\n",
    "        class_percent = c0/(c0+c1)\n",
    "        if auto_balancing:\n",
    "            if class_percent<=0.4 or class_percent>=0.6:\n",
    "                train_balancing=True\n",
    "            else:\n",
    "                train_balancing=False\n",
    "        #\n",
    "        m3_runT = []\n",
    "        m5_runT = []\n",
    "        for mode in mode_list: # 0:3-5, 1:4-6\n",
    "            if mode==1: continue\n",
    "            if mode==0:\n",
    "                doc2vec_rnn = True\n",
    "                m3_stacking = False\n",
    "                doc2vec_avg = False\n",
    "                multi_layer_LSTM = False\n",
    "            elif mode ==1:\n",
    "                doc2vec_rnn = False \n",
    "                m3_stacking = False\n",
    "                doc2vec_avg = True\n",
    "                multi_layer_LSTM = False\n",
    "            for i in range(6,11):\n",
    "                totalLength = len(all_data)\n",
    "                print('Total length: ', totalLength)\n",
    "                splitLength = int(totalLength / (numCV + 1))\n",
    "                print('emb_std:',emb_std)\n",
    "                print('seq_std:', seq_std)\n",
    "                print('train_balancing', train_balancing)\n",
    "                # Split cross validation set\n",
    "                print ('CV',i)\n",
    "                if test_mode:\n",
    "                    train_data = all_data[:30]#i*splitLength]\n",
    "                    train_history = all_history[:30]#i*splitLength]\n",
    "                    train_3to1 = sum([len(r) for r in train_data])       \n",
    "                    train_time = all_time[:train_3to1]#i*splitLength]\n",
    "                    train_workday = all_workday[:train_3to1]\n",
    "                    train_recentday = all_recentday[:train_3to1]\n",
    "                    train_openday = all_openday[:train_3to1]\n",
    "                    train_activitycnt = all_activitycnt[:train_3to1]\n",
    "                    train_cc = all_cc[:train_3to1]\n",
    "                    train_writer = all_writer[:train_3to1]\n",
    "                    train_bugid = all_bugid[:train_3to1]\n",
    "                else:\n",
    "                    train_data = all_data[:i*splitLength]\n",
    "                    train_history = all_history[:i*splitLength]\n",
    "                    train_3to1 = sum([len(r) for r in train_data])       \n",
    "                    train_time = all_time[:train_3to1]#i*splitLength]\n",
    "                    train_workday = all_workday[:train_3to1]\n",
    "                    train_recentday = all_recentday[:train_3to1]\n",
    "                    train_openday = all_openday[:train_3to1]\n",
    "                    train_activitycnt = all_activitycnt[:train_3to1]\n",
    "                    train_cc = all_cc[:train_3to1]\n",
    "                    train_writer = all_writer[:train_3to1]\n",
    "                    train_bugid = all_bugid[:train_3to1]\n",
    "\n",
    "                # ===================================================================== \n",
    "                updated_train_data = []    \n",
    "                updated_train_history = []    \n",
    "                updated_train_time = []\n",
    "                #updated_train_stream = []\n",
    "                #updated_train_workday = []\n",
    "                #updated_train_bugid = []\n",
    "\n",
    "                j=0\n",
    "                for bug1, bug2 in zip(train_data, train_history):\n",
    "                    train_data_list = []\n",
    "                    train_history_list = []\n",
    "                    if len(bug1)>=min_activity_len:\n",
    "                        for act1, act2 in zip(bug1, bug2):\n",
    "                            current_train_filter = [word for word in act1 if word in vocabulary]\n",
    "                            his_current_train_filter = [word for word in act2 if word in vocabulary]\n",
    "                            train_data_list.append(current_train_filter)\n",
    "                            train_history_list.append(his_current_train_filter)\n",
    "                            j+=1\n",
    "                        updated_train_data.append(train_data_list)\n",
    "                        updated_train_history.append(train_history_list)\n",
    "                    else:\n",
    "                        j+=len(bug1)\n",
    "\n",
    "                del train_data, train_history, #train_bugid, train_workday\n",
    "                gc.collect()\n",
    "\n",
    "                # ==================================================================== \n",
    "                updated_train_time = [0 if x<=fixtime_thr[0] else 1 if x<=fixtime_thr[1] else 2 if x<=fixtime_thr[2] else 3 if x<=fixtime_thr[3] else 4 if x<=fixtime_thr[4] else 5 if x<=fixtime_thr[5] else 6 for x in train_time]\n",
    "              \n",
    "                curr_split = total_split[data_name[path]][0]\n",
    "                updated_train_workday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 for x in train_workday]\n",
    "                updated_train_recentday = []\n",
    "                for x in train_recentday:\n",
    "                    tmp = 0 \n",
    "                    for j,y in enumerate(x):\n",
    "                        tmp+= pow(2,j)*y\n",
    "                    updated_train_recentday.append(tmp) \n",
    "                curr_split = total_split[data_name[path]][1]\n",
    "                updated_train_openday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in train_openday]\n",
    "                curr_split = total_split[data_name[path]][2]\n",
    "                updated_train_activitycnt = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in train_activitycnt]\n",
    "                curr_split = total_split[data_name[path]][3]\n",
    "                updated_train_cc = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 for x in train_cc]\n",
    "                updated_train_writer = train_writer\n",
    "                updated_train_bugid = train_bugid\n",
    "                updated_train_data_1d = [y for x in updated_train_data for y in x]\n",
    "                updated_train_history_1d = [y for x in updated_train_history for y in x]\n",
    "                train_col_size = [len(x) for x in updated_train_data]\n",
    "                train_n = len(updated_train_data_1d)\n",
    "                train_df =pd.DataFrame({'x1': updated_train_data_1d, 'x2': updated_train_history_1d, \n",
    "                                        's1': updated_train_workday, 's2':updated_train_recentday, 's3':updated_train_activitycnt,\n",
    "                                        's4':updated_train_cc, 's5':updated_train_writer, 's6':updated_train_openday,\n",
    "                                        'bugid':updated_train_bugid,\n",
    "                                        'y1': train_time, 'y2':updated_train_time})\n",
    "                #print(train_df[:10])\n",
    "                del updated_train_time, updated_train_history, updated_train_data, updated_train_data_1d, updated_train_history_1d\n",
    "                del updated_train_workday, updated_train_recentday, updated_train_activitycnt, updated_train_cc, updated_train_writer, updated_train_bugid\n",
    "                gc.collect()\n",
    "\n",
    "                # =============================\n",
    "                # BALANCE RESAMPLING\n",
    "                if train_balancing:\n",
    "                    class_count = train_df.y2.value_counts()\n",
    "                    print('\\nClass 0:', class_count[0])\n",
    "                    print('Class 1:', class_count[1])\n",
    "                    df_class_0 = train_df[train_df['y2']==0]\n",
    "                    df_class_1 = train_df[train_df['y2']==1]\n",
    "                    if class_count[0]<class_count[1]:\n",
    "                        under_class_cnt = class_count[0]\n",
    "                        df_class_1 = df_class_1.sample(under_class_cnt)\n",
    "                    else:\n",
    "                        under_class_cnt = class_count[1]\n",
    "                        df_class_0 = df_class_0.sample(under_class_cnt)\n",
    "                    train_df = pd.concat([df_class_0, df_class_1], axis=0)\n",
    "                    train_df = train_df.sort_index()\n",
    "                    train_n = len(train_df)\n",
    "                    print(train_df[:10],'\\n')\n",
    "                    print('\\nRandom under-sampling:')\n",
    "                    print(train_df.y2.value_counts(),'\\n')\n",
    "                    # new train_col_size\n",
    "                    cnt=0\n",
    "                    train_col_size = []\n",
    "                    prev_id = train_df.iloc[0]['bugid']\n",
    "                    for x in train_df.bugid.values:\n",
    "                        if prev_id != x:\n",
    "                            train_col_size.append(cnt)\n",
    "                            cnt = 0\n",
    "                        prev_id = x\n",
    "                        cnt+=1\n",
    "                    train_col_size.append(cnt)\n",
    "\n",
    "                # ===================================================================== \n",
    "                X_train_last = np.zeros(shape=[train_n, max_sentence_len, embed_size_word2vec], dtype='float32') # len(updated_train_data) # train_len*2\n",
    "                X_train_history = np.zeros(shape=[train_n, max_his_len, embed_size_word2vec], dtype='float32') # len(updated_train_history)\n",
    "                Y_train = np.empty(shape=[train_n,1], dtype='int32') # len(updated_train_time)\n",
    "\n",
    "                j=0\n",
    "                for curr_row1, curr_row2 in zip(train_df.x1.values, train_df.x2.values):\n",
    "                    if len(curr_row1)>max_sentence_len:\n",
    "                        start_loc = len(curr_row1) - max_sentence_len\n",
    "                    else: \n",
    "                        start_loc = 0\n",
    "                    sequence_cnt = 0    \n",
    "                    for item1 in curr_row1[start_loc:]:\n",
    "                        if item1 in vocabulary:\n",
    "                            X_train_last[j, sequence_cnt, :] = wordvec_model[item1] \n",
    "                            sequence_cnt = sequence_cnt + 1                \n",
    "                            if sequence_cnt == max_sentence_len-1:\n",
    "                                break  \n",
    "                    for k in range(sequence_cnt, max_sentence_len):\n",
    "                        X_train_last[j, k, :] = np.zeros((1,embed_size_word2vec))   \n",
    "                    if len(curr_row2)>max_his_len:\n",
    "                        start_loc = len(curr_row2) - max_his_len\n",
    "                    else: \n",
    "                        start_loc = 0\n",
    "                    sequence_cnt = 0\n",
    "                    for item2 in curr_row2[start_loc:]:\n",
    "                        if item2 in vocabulary:\n",
    "                            X_train_history[j, sequence_cnt, :] = wordvec_model[item2] \n",
    "                            sequence_cnt = sequence_cnt + 1                \n",
    "                            if sequence_cnt == max_his_len-1:\n",
    "                                    break \n",
    "                    for k in range(sequence_cnt, max_his_len):\n",
    "                        X_train_history[j, k, :] = np.zeros((1,embed_size_word2vec))\n",
    "                    Y_train[j] = train_df.y2.values[j]\n",
    "                    j+=1\n",
    "                y_train = np_utils.to_categorical(Y_train, label_num)\n",
    "\n",
    "                if doc2vec_avg:\n",
    "                    X_train_doc = np.zeros(shape=[train_n, embed_size_word2vec], dtype='float32')\n",
    "                    X_train_his_doc = np.zeros(shape=[train_n, embed_size_word2vec], dtype='float32')\n",
    "                    for j,words in enumerate(X_train_last):\n",
    "                        cnt=0\n",
    "                        smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                        for word in words:\n",
    "                            if not np.any(word): # All-zero element?\n",
    "                                break\n",
    "                            smt+=word\n",
    "                            cnt+=1\n",
    "                        if not np.any(smt):\n",
    "                            avg = smt\n",
    "                        else:\n",
    "                            avg = smt/cnt\n",
    "                        X_train_doc[j] = avg\n",
    "                    for j,words in enumerate(X_train_history):\n",
    "                        cnt=0\n",
    "                        smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                        for word in words:\n",
    "                            if not np.any(word): # All-zero element?\n",
    "                                break\n",
    "                            smt+=word\n",
    "                            cnt+=1\n",
    "                        if not np.any(smt):\n",
    "                            avg = smt\n",
    "                        else:\n",
    "                            avg = smt/cnt\n",
    "                        X_train_his_doc[j] = avg\n",
    "\n",
    "                    X_train_avg = np.concatenate((X_train_doc, X_train_his_doc),axis=1)\n",
    "                    del X_train_last, X_train_history\n",
    "                    gc.collect()\n",
    "\n",
    "                # Statistic\n",
    "                X_train_workday = np.asarray(train_df.s1.values, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][0])+1\n",
    "                X_train_workday = np_utils.to_categorical(X_train_workday, class_num)\n",
    "                X_train_openday = np.asarray(train_df.s6.values, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][1])+1\n",
    "                X_train_openday = np_utils.to_categorical(X_train_openday, class_num)\n",
    "                X_train_recentday = np.asarray(train_df.s2.values, dtype=np.float32)\n",
    "                class_num = 8\n",
    "                X_train_recentday = np_utils.to_categorical(X_train_recentday, class_num)\n",
    "                X_train_activitycnt = np.asarray(train_df.s3.values, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][2])+1\n",
    "                X_train_activitycnt = np_utils.to_categorical(X_train_activitycnt, class_num)\n",
    "                X_train_cc = np.asarray(train_df.s4.values, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][3])+1\n",
    "                X_train_cc = np_utils.to_categorical(X_train_cc, class_num)\n",
    "                X_train_writer = np.array([np.array(x, dtype=np.float32) for x in train_df.s5.values])\n",
    "\n",
    "                X_train_stat = np.concatenate((X_train_workday, X_train_recentday),axis=1)\n",
    "                X_train_stat = np.concatenate((X_train_stat, X_train_openday),axis=1)\n",
    "                X_train_stat = np.concatenate((X_train_stat, X_train_activitycnt),axis=1)\n",
    "                X_train_stat = np.concatenate((X_train_stat, X_train_cc),axis=1)\n",
    "                X_train_stat = np.concatenate((X_train_stat, X_train_writer),axis=1)\n",
    "\n",
    "                # Model\n",
    "                if doc2vec_rnn:\n",
    "                    h1=256\n",
    "                    h2 = 32\n",
    "                    last_input = Input(shape=(max_sentence_len, embed_size_word2vec), dtype='float32')\n",
    "                    mask_1 = Masking(mask_value=0)(last_input)\n",
    "                    forwards_1 = LSTM(h1, name='forwards_1')(mask_1)\n",
    "                    backwards_1 = LSTM(h1, go_backwards=True, name='backwords_1')(mask_1)         \n",
    "                    merged_1 = keras.layers.concatenate([forwards_1, backwards_1], axis=-1)\n",
    "                    after_dp_1= Dropout(0.5, name='after_dp_1')(merged_1)\n",
    "                    tmp_output_1 = Dense(label_num, activation='softmax', name='tmp_output_1')(after_dp_1)\n",
    "\n",
    "                    history_input = Input(shape=(max_his_len, embed_size_word2vec), dtype='float32')\n",
    "                    mask_2 = Masking(mask_value=0)(history_input)\n",
    "                    forwards_2 = LSTM(h2, name='forwards_2')(mask_2)\n",
    "                    backwards_2 = LSTM(h2, go_backwards=True, name='backwards_2')(mask_2)\n",
    "                    merged_2 = keras.layers.concatenate([forwards_2, backwards_2], axis=-1)\n",
    "                    after_dp_2 = Dropout(0.5, name='after_dp_2')(merged_2)\n",
    "                    tmp_output_2 = Dense(label_num, activation='softmax', name='tmp_output_2')(after_dp_2)\n",
    "\n",
    "                    # stat\n",
    "                    stat_input = Input(shape=(len(X_train_stat[0]),), dtype='float32')\n",
    "\n",
    "                    middle_merge = keras.layers.concatenate([after_dp_1, after_dp_2, stat_input], axis=-1)\n",
    "                    middle_dense = Dense(380, name='middle_dense_1')(middle_merge) \n",
    "                    middle_dense = LeakyReLU(alpha=0.3)(middle_dense)\n",
    "                    middle_dense = Dense(450, name='middle_dense_2')(middle_dense) \n",
    "                    middle_dense = LeakyReLU(alpha=0.2)(middle_dense)\n",
    "                    middle_dense = Dense(200, name='middle_dense_3', activation='tanh')(middle_dense) \n",
    "                    #middle_dense = LeakyReLU(alpha=0.5)(middle_dense)\n",
    "                    #last_dense = Dense(100, name='middle_dense_4')(middle_dense) \n",
    "                    #last_dense = LeakyReLU(alpha=0.1)(last_dense)\n",
    "                    output = Dense(label_num, activation='softmax', name='output')(middle_dense)\n",
    "                    if m3_stacking:\n",
    "                        user_model = Model(input=last_input, output = tmp_output_1)\n",
    "                        sys_model = Model(input=history_input, output=tmp_output_2)\n",
    "                    embedding_size = 200\n",
    "                    m3 = Model(input=[last_input, history_input, stat_input], output=output)    \n",
    "\n",
    "                elif doc2vec_avg:\n",
    "                    docAvg_input = Input(shape=(len(X_train_avg[0]),), dtype='float32')\n",
    "                    stat_input = Input(shape=(len(X_train_stat[0]),), dtype='float32')\n",
    "                    concat_input = keras.layers.concatenate([docAvg_input, stat_input], axis=-1)\n",
    "                    middle_dense = Dense(230, name='middle_dense_1')(concat_input) \n",
    "                    middle_dense = LeakyReLU(alpha=0.3)(middle_dense)\n",
    "                    middle_dense = Dense(300, name='middle_dense_2')(middle_dense) \n",
    "                    middle_dense = LeakyReLU(alpha=0.2)(middle_dense)\n",
    "                    middle_dense = Dense(180, name='middle_dense_3')(middle_dense) \n",
    "                    middle_dense = LeakyReLU(alpha=0.5)(middle_dense)\n",
    "                    last_dense = Dense(100, name='middle_dense_4')(middle_dense) \n",
    "                    last_dense = LeakyReLU(alpha=0.1)(last_dense)\n",
    "                    output = Dense(label_num, activation='softmax', name='output')(last_dense)\n",
    "\n",
    "                    embedding_size = 180\n",
    "                    m3 = Model(input=[docAvg_input, stat_input], output=output)    \n",
    "\n",
    "                adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "                m3.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "                # TRAIN MODEL\n",
    "                es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "                if doc2vec_rnn:\n",
    "                    if m3_stacking:\n",
    "                        # Except for X_train_last == []\n",
    "                        mask = [0 for x in range(embed_size_word2vec)]\n",
    "                        condition_bool =  np.ndarray.all((X_train_last==mask), axis=2).all(axis=1)\n",
    "                        X_train_last_mask = np.delete(X_train_last, np.where(condition_bool), 0)\n",
    "                        y_train_mask = np.delete(y_train, np.where(np.where(condition_bool)), 0)\n",
    "                        X_train_last_mask, X_valid_last_mask, y_train_mask, y_valid_mask = train_test_split(X_train_last_mask, y_train_mask, test_size=0.15, shuffle= True)\n",
    "\n",
    "                        print('\\n******************* User-RNN *******************')\n",
    "                        user_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "                        user_model.summary()\n",
    "                        hist = user_model.fit(X_train_last_mask, y_train_mask, batch_size=batch_size, \n",
    "                                     validation_data = (X_valid_last_mask, y_valid_mask), epochs=20, callbacks=[es])\n",
    "                        print(hist.history)\n",
    "\n",
    "                        # Except for X_train_history == []\n",
    "                        condition_bool =  np.ndarray.all((X_train_history==mask), axis=2).all(axis=1)\n",
    "                        X_train_his_mask = np.delete(X_train_history, np.where(condition_bool), 0)\n",
    "                        y_train_mask = np.delete(y_train, np.where(condition_bool), 0)\n",
    "                        X_train_his_mask, X_valid_his_mask, y_train_mask, y_valid_mask = train_test_split(X_train_his_mask, y_train_mask, test_size=0.15, shuffle= True)\n",
    "\n",
    "                        print('\\n\\n******************* Sys-RNN *******************')\n",
    "                        sys_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "                        sys_model.summary()\n",
    "                        hist = sys_model.fit(X_train_his_mask, y_train_mask, batch_size=batch_size, \n",
    "                                     validation_data = (X_valid_his_mask, y_valid_mask), epochs=20, callbacks=[es])\n",
    "                        print(hist.history)\n",
    "\n",
    "                        # Save sys_log weights\n",
    "                        if not save_off:\n",
    "                            model_json = user_model.to_json()\n",
    "                            model_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.json\"\n",
    "                            weight_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                            with open(model_name,\"w\") as json_file :\n",
    "                                json_file.write(model_json)\n",
    "                                user_model.save_weights(weight_name)\n",
    "                            print(\"Saved model to disk\\n\\n\\n\")\n",
    "\n",
    "                            model_json = sys_model.to_json()\n",
    "                            model_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.json\"\n",
    "                            weight_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                            with open(model_name,\"w\") as json_file :\n",
    "                                json_file.write(model_json)\n",
    "                                sys_model.save_weights(weight_name)\n",
    "                            print(\"Saved model to disk\\n\\n\\n\")\n",
    "                            del user_model, sys_model\n",
    "\n",
    "                        # Load weights (user_comment + sys_log)\n",
    "                        weight_name = data_name[path]+\"_M3_userRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                        m3.load_weights(weight_name, by_name=True)\n",
    "                        weight_name = data_name[path]+\"_M3_sysRNN_word2vec200_cv\"+str(i)+\"model.h5\"\n",
    "                        m3.load_weights(weight_name, by_name=True)\n",
    "\n",
    "                    print('\\n\\n******************* M3-RNN *******************')  \n",
    "                    if emb_load:\n",
    "                        pass\n",
    "                    else:\n",
    "                        X_train_last_sp, X_valid_last, X_train_history_sp, X_valid_history, X_train_stat_sp, X_valid_stat, y_train_sp, y_valid = train_test_split(X_train_last, X_train_history, X_train_stat, y_train, test_size=0.15, shuffle= True)\n",
    "                        m3.summary()\n",
    "                        start = time.time()\n",
    "                        hist = m3.fit([X_train_last_sp, X_train_history_sp, X_train_stat_sp], y_train_sp, batch_size=batch_size, \n",
    "                                         validation_data = ([X_valid_last, X_valid_history, X_valid_stat], y_valid), epochs=20, callbacks=[es])\n",
    "                        end = time.time()\n",
    "                        m3_runT.append(int(end-start))\n",
    "                        print(\" ********* emb%d-M3 Learning time: %dsec ********* \" %(emb_std,int(end-start)))\n",
    "                        del X_train_last_sp, X_train_history_sp, X_train_stat_sp, y_train_sp\n",
    "                        del X_valid_last, X_valid_history, X_valid_stat, y_valid\n",
    "                        gc.collect()\n",
    "                        if not save_off:\n",
    "                            # Save model\n",
    "                            model_json = m3.to_json()\n",
    "                            model_name = data_name[path]+\"_M3_limit100_class\"+str(emb_std)+\"_cv\"+str(i)+\".json\"\n",
    "                            weight_name = data_name[path]+\"_M3_limit100_class\"+str(emb_std)+\"_cv\"+str(i)+\".h5\"\n",
    "                            with open(model_name,\"w\") as json_file :\n",
    "                                json_file.write(model_json)\n",
    "                                m3.save_weights(weight_name)\n",
    "                            print(\"Saved model to disk\\n\\n\\n\")\n",
    "\n",
    "                    print('\\n\\n******************* M5- seqence RNN *******************')\n",
    "                    model_1 = Model(input=[last_input, history_input, stat_input], output=middle_dense) \n",
    "                    #pre_path = '/home/hwoo/suin/deepTriage/0 our_model/'\n",
    "                    weight_name = data_name[path]+\"_M3_tanh_limit100_class\"+str(emb_std)+\"_cv\"+str(i)+\".h5\"\n",
    "                    model_1.load_weights(weight_name, by_name=True)\n",
    "                    adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "                    model_1.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])                    \n",
    "\n",
    "                    embedding_train_output = model_1.predict([X_train_last, X_train_history, X_train_stat])\n",
    "                    del X_train_last, X_train_history, X_train_stat\n",
    "                    gc.collect()\n",
    "\n",
    "                    X_train_activity = np.zeros(shape=[len(train_col_size), max_activity_len, embedding_size], dtype='float32') \n",
    "                    y_train_activity = np.full((len(train_col_size), max_activity_len, label_num), -1, dtype='int32') \n",
    "                    act_idx = 0 # 200d_activity_output row\n",
    "\n",
    "                    for j, col_size in enumerate(train_col_size):\n",
    "                        for p in range(col_size):\n",
    "                            X_train_activity[j,p,:] = embedding_train_output[act_idx+p]\n",
    "                            y_train_activity[j,p,:] = np.zeros(label_num, dtype='int32')\n",
    "                            y_train_activity[j,p,train_df.y2.values[act_idx+p]] = 1\n",
    "                        act_idx += col_size\n",
    "                    del embedding_train_output \n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Model\n",
    "                    if ResLSTM:\n",
    "                        data_input = Input(shape=(max_activity_len, embedding_size), dtype='float32')\n",
    "                        output = make_residual_lstm_layers(data_input, rnn_width=embedding_size, rnn_depth=8, rnn_dropout=0.2)\n",
    "                        output = TimeDistributed(Dense(label_num, activation='softmax'))(output) \n",
    "                        model = Model(input=data_input, output=output)\n",
    "                    else:\n",
    "                        activity_input = Input(shape=(max_activity_len, embedding_size), dtype='float32')\n",
    "                        mask = Masking(mask_value=0)(activity_input)\n",
    "                        forwards = LSTM(256, name='forwards', return_sequences=True)(mask)\n",
    "                        forwards = Dropout(0.5)(forwards)\n",
    "                        #forwards = LSTM(128, return_sequences=True)(forwards)\n",
    "                        #forwards = Dropout(0.3)(forwards)\n",
    "                        output = TimeDistributed(Dense(label_num, activation='softmax'))(forwards)\n",
    "                        model = Model(input=activity_input, output=output)  \n",
    "                        \n",
    "                    adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "                    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "                    model.summary()\n",
    "                \n",
    "                    # TRAIN MODEL\n",
    "                    es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "                    X_train_activity, X_valid_activity, y_train_activity, y_valid_activity = train_test_split(X_train_activity, y_train_activity, test_size=0.15, shuffle= True)\n",
    "                    start = time.time()\n",
    "                    hist = model.fit(X_train_activity, y_train_activity, batch_size=batch_size, \n",
    "                                     validation_data = (X_valid_activity, y_valid_activity), epochs=20, callbacks=[es])\n",
    "                    end = time.time()\n",
    "                    m5_runT.append(int(end-start))\n",
    "                    print(\" ********* seq%d-M5 Learning time: %dsec ********* \" %(seq_std,int(end-start)))\n",
    "                    train_result = hist.history\n",
    "                    print('\\nTrain_result\\n')\n",
    "                    print(train_result)\n",
    "                    del X_train_activity, X_valid_activity, y_train_activity, y_valid_activity\n",
    "                    gc.collect()\n",
    "\n",
    "                    if not save_off:\n",
    "                        # Save model\n",
    "                        model_json = model.to_json()\n",
    "                        model_name = data_name[path]+\"_newM5_limit100_emb\"+str(emb_std)+\"_seq\"+str(seq_std)+\"_cv\"+str(i)+\".json\"\n",
    "                        weight_name = data_name[path]+\"_newM5_limit100_emb\"+str(emb_std)+\"_seq\"+str(seq_std)+\"_cv\"+str(i)+\".h5\"\n",
    "                        with open(model_name,\"w\") as json_file :\n",
    "                            json_file.write(model_json)\n",
    "                            model.save_weights(weight_name)\n",
    "                        print(\"Saved model to disk\\n\\n\\n\")  \n",
    "\n",
    "                #========================================================================================\n",
    "                # TEST DATA\n",
    "                #========================================================================================\n",
    "                if test_mode:\n",
    "                    test_data = all_data[30:50]#i*splitLength:(i+1)*splitLength] \n",
    "                    test_history = all_history[30:50]#i*splitLength:(i+1)*splitLength]\n",
    "                    test_3to1 = sum([len(r) for r in test_data])\n",
    "                    test_time = all_time[train_3to1:train_3to1+test_3to1]#i*splitLength:(i+1)*splitLength]\n",
    "                    #test_stream = all_stream[train_3to1 : train_3to1+test_3to1]\n",
    "                    test_workday = all_workday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_recentday = all_recentday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_openday = all_openday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_activitycnt = all_activitycnt[train_3to1:train_3to1+test_3to1]\n",
    "                    test_cc = all_cc[train_3to1:train_3to1+test_3to1]\n",
    "                    test_writer = all_writer[train_3to1:train_3to1+test_3to1]\n",
    "                else:\n",
    "                    test_data = all_data[i*splitLength:(i+1)*splitLength]\n",
    "                    test_history = all_history[i*splitLength:(i+1)*splitLength]\n",
    "                    test_3to1 = sum([len(r) for r in test_data])       \n",
    "                    test_time = all_time[train_3to1 : train_3to1+test_3to1]#i*splitLength]\n",
    "                    #test_stream = all_stream[train_3to1:train_3to1+test_3to1]\n",
    "                    test_workday = all_workday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_recentday = all_recentday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_openday = all_openday[train_3to1:train_3to1+test_3to1]\n",
    "                    test_activitycnt = all_activitycnt[train_3to1:train_3to1+test_3to1]\n",
    "                    test_cc = all_cc[train_3to1:train_3to1+test_3to1]\n",
    "                    test_writer = all_writer[train_3to1:train_3to1+test_3to1]\n",
    "\n",
    "                # ===================================================================== \n",
    "                updated_test_data = []    \n",
    "                updated_test_history = []    \n",
    "                updated_test_time = []\n",
    "                #updated_test_stream = []\n",
    "                #updated_test_workday = []\n",
    "                #updated_test_bugid = []\n",
    "\n",
    "                j=0\n",
    "                for bug1, bug2 in zip(test_data, test_history):\n",
    "                    test_data_list = []\n",
    "                    test_history_list = []\n",
    "                    if len(bug1)>=min_activity_len:\n",
    "                        for act1, act2 in zip(bug1, bug2):\n",
    "                            current_test_filter = [word for word in act1 if word in vocabulary]\n",
    "                            his_current_test_filter = [word for word in act2 if word in vocabulary]\n",
    "                            test_data_list.append(current_test_filter)\n",
    "                            test_history_list.append(his_current_test_filter)\n",
    "                            updated_test_time.append(test_time[j])\n",
    "                            #updated_test_stream.append(test_stream[j])\n",
    "                            #updated_test_workday.append(test_workday[j])\n",
    "                            #updated_test_bugid.append(test_bugid[j])\n",
    "                            j+=1\n",
    "                        updated_test_data.append(test_data_list)\n",
    "                        updated_test_history.append(test_history_list)\n",
    "                    else:\n",
    "                        j+=len(bug1)\n",
    "\n",
    "                del test_data, test_history, #test_bugid, test_workday\n",
    "                gc.collect()\n",
    "\n",
    "                # ===================================================================== \n",
    "                updated_test_time = [0 if x<=fixtime_thr[0] else 1 if x<=fixtime_thr[1] else 2 if x<=fixtime_thr[2] else 3 if x<=fixtime_thr[3] else 4 if x<=fixtime_thr[4] else 5 if x<=fixtime_thr[5] else 6 for x in test_time]\n",
    "\n",
    "                curr_split = total_split[data_name[path]][0]\n",
    "                updated_test_workday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 for x in test_workday]\n",
    "                updated_test_recentday = []\n",
    "                for x in test_recentday:\n",
    "                    tmp = 0 \n",
    "                    for j,y in enumerate(x):\n",
    "                        tmp+= pow(2,j)*y\n",
    "                    updated_test_recentday.append(tmp) \n",
    "                curr_split = total_split[data_name[path]][1]\n",
    "                updated_test_openday = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in test_openday]\n",
    "                curr_split = total_split[data_name[path]][2]\n",
    "                updated_test_activitycnt = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 if x<=curr_split[3] else 4 for x in test_activitycnt]\n",
    "                curr_split = total_split[data_name[path]][3]\n",
    "                updated_test_cc = [0 if x<=curr_split[0] else 1 if x<=curr_split[1] else 2 if x<=curr_split[2] else 3 for x in test_cc]\n",
    "                updated_test_writer = test_writer\n",
    "              \n",
    "                updated_test_data_1d = [y for x in updated_test_data for y in x]\n",
    "                updated_test_history_1d = [y for x in updated_test_history for y in x]\n",
    "                test_col_size = [len(x) for x in updated_test_data]\n",
    "                test_n = len(updated_test_data_1d)\n",
    "                del updated_test_data, updated_test_history, #test_workday, test_recentday,test_activitycnt\n",
    "                gc.collect()\n",
    "\n",
    "                # ===================================================================== \n",
    "                X_test_last = np.zeros(shape=[test_n, max_sentence_len, embed_size_word2vec], dtype='float32') # len(updated_test_data) # test_len*2\n",
    "                X_test_history = np.zeros(shape=[test_n, max_his_len, embed_size_word2vec], dtype='float32') # len(updated_test_history)\n",
    "                #X_test_stream = np.empty(shape=[test_n, limit_day], dtype='float32')\n",
    "                #X_test_workday = np.empty(shape=[test_n, max_workday], dtype='float32')\n",
    "                Y_test = np.empty(shape=[test_n,1], dtype='int32') # len(updated_test_time)\n",
    "\n",
    "                j=0\n",
    "                for curr_row1, curr_row2 in zip(updated_test_data_1d, updated_test_history_1d):\n",
    "                    if len(curr_row1)>max_sentence_len:\n",
    "                        start_loc = len(curr_row1) - max_sentence_len\n",
    "                    else: \n",
    "                        start_loc = 0\n",
    "                    sequence_cnt = 0    \n",
    "                    for item1 in curr_row1[start_loc:]:\n",
    "                        if item1 in vocabulary:\n",
    "                            X_test_last[j, sequence_cnt, :] = wordvec_model[item1] \n",
    "                            sequence_cnt = sequence_cnt + 1                \n",
    "                            if sequence_cnt == max_sentence_len-1:\n",
    "                                break  \n",
    "                    for k in range(sequence_cnt, max_sentence_len):\n",
    "                        X_test_last[j, k, :] = np.zeros((1,embed_size_word2vec))   \n",
    "                    if len(curr_row2)>max_his_len:\n",
    "                        start_loc = len(curr_row2) - max_his_len\n",
    "                    else: \n",
    "                        start_loc = 0\n",
    "                    sequence_cnt = 0\n",
    "                    for item2 in curr_row2[start_loc:]:\n",
    "                        if item2 in vocabulary:\n",
    "                            X_test_history[j, sequence_cnt, :] = wordvec_model[item2] \n",
    "                            sequence_cnt = sequence_cnt + 1                \n",
    "                            if sequence_cnt == max_his_len-1:\n",
    "                                    break \n",
    "                    for k in range(sequence_cnt, max_his_len):\n",
    "                        X_test_history[j, k, :] = np.zeros((1,embed_size_word2vec))\n",
    "                    Y_test[j] = updated_test_time[j]\n",
    "                    j+=1\n",
    "                    \n",
    "                if doc2vec_avg:\n",
    "                    X_test_doc = np.zeros(shape=[test_n, embed_size_word2vec], dtype='float32')\n",
    "                    X_test_his_doc = np.zeros(shape=[test_n, embed_size_word2vec], dtype='float32')\n",
    "                    for j,words in enumerate(X_test_last):\n",
    "                        cnt=0\n",
    "                        smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                        for word in words:\n",
    "                            if not np.any(word): # All-zero element?\n",
    "                                break\n",
    "                            smt+=word\n",
    "                            cnt+=1\n",
    "                        if not np.any(smt):\n",
    "                            avg = smt\n",
    "                        else:\n",
    "                            avg = smt/cnt\n",
    "                        X_test_doc[j] = avg\n",
    "                    for j,words in enumerate(X_test_history):\n",
    "                        cnt=0\n",
    "                        smt = np.asarray([0.0 for p in range(embed_size_word2vec)])\n",
    "                        for word in words:\n",
    "                            if not np.any(word): # All-zero element?\n",
    "                                break\n",
    "                            smt+=word\n",
    "                            cnt+=1\n",
    "                        if not np.any(smt):\n",
    "                            avg = smt\n",
    "                        else:\n",
    "                            avg = smt/cnt\n",
    "                        X_test_his_doc[j] = avg\n",
    "\n",
    "                    X_test_avg = np.concatenate((X_test_doc, X_test_his_doc),axis=1)\n",
    "                    del X_test_last, X_test_history\n",
    "                    gc.collect()\n",
    "\n",
    "                # Statistic\n",
    "                X_test_workday = np.asarray(updated_test_workday, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][0])+1\n",
    "                X_test_workday = np_utils.to_categorical(X_test_workday, class_num)\n",
    "                X_test_openday = np.asarray(updated_test_openday, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][1])+1\n",
    "                X_test_openday = np_utils.to_categorical(X_test_openday, class_num)\n",
    "                X_test_recentday = np.asarray(updated_test_recentday, dtype=np.float32)\n",
    "                class_num = 8\n",
    "                X_test_recentday = np_utils.to_categorical(X_test_recentday, class_num)\n",
    "                X_test_activitycnt = np.asarray(updated_test_activitycnt, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][2])+1\n",
    "                X_test_activitycnt = np_utils.to_categorical(X_test_activitycnt, class_num)\n",
    "                X_test_cc = np.asarray(updated_test_cc, dtype=np.float32)\n",
    "                class_num = len(total_split[data_name[path]][3])+1\n",
    "                X_test_cc = np_utils.to_categorical(X_test_cc, class_num)\n",
    "                X_test_writer = np.asarray(updated_test_writer, dtype=np.float32)\n",
    "\n",
    "                X_test_stat = np.concatenate((X_test_workday, X_test_recentday),axis=1)\n",
    "                X_test_stat = np.concatenate((X_test_stat, X_test_openday),axis=1)\n",
    "                X_test_stat = np.concatenate((X_test_stat, X_test_activitycnt),axis=1)\n",
    "                X_test_stat = np.concatenate((X_test_stat, X_test_cc),axis=1)\n",
    "                X_test_stat = np.concatenate((X_test_stat, X_test_writer),axis=1)\n",
    "\n",
    "                # ===================================================================== \n",
    "                if doc2vec_rnn:\n",
    "                    embedding_test_output = model_1.predict([X_test_last, X_test_history, X_test_stat])\n",
    "                    del model_1\n",
    "                    X_test_activity_orig = np.zeros(shape=[len(test_col_size), max_activity_len, embedding_size], dtype='float32') \n",
    "                    y_test_activity_orig = np.full((len(test_col_size), max_activity_len, 1), -1, dtype='int32') \n",
    "                    act_idx = 0 # 200d_activity_output row\n",
    "                    for j, col_size in enumerate(test_col_size):\n",
    "                        for p in range(col_size):\n",
    "                            X_test_activity_orig[j,p,:] = embedding_test_output[act_idx+p]\n",
    "                            y_test_activity_orig[j,p,:] = updated_test_time[act_idx+p]\n",
    "                        act_idx += col_size\n",
    "                \n",
    "                #========================================================================================\n",
    "                # M3 PREDICT & ACCURACY \n",
    "                #========================================================================================\n",
    "                if mode==0:\n",
    "                    print('\\n\\n******************* M3 - Embedding RNN *******************') \n",
    "                elif mode==1:\n",
    "                    print('\\n\\n******************* M4 - Embedding RNN *******************') \n",
    "                if doc2vec_rnn:\n",
    "                    predict = m3.predict([X_test_last, X_test_history, X_test_stat])  \n",
    "                    del X_test_last, X_test_history, X_test_stat\n",
    "                elif doc2vec_avg:\n",
    "                    predict = m3.predict([X_test_avg, X_test_stat]) \n",
    "                    del X_test_avg, X_test_stat\n",
    "                predictY = np.argmax(predict, axis=1)\n",
    "                corrects = np.nonzero(predictY.reshape((-1,1)) == Y_test) #.reshape((-1,))\n",
    "                accu = len(corrects[0])/len(Y_test)\n",
    "                print(\"Total test accuracy : %.2f\" % (accu*100))\n",
    "                print('\\nConfusion Matrix')\n",
    "                cm = confusion_matrix(Y_test, predictY)\n",
    "                print(cm)\n",
    "                total = sum(cm[0])+sum(cm[1])\n",
    "                #print('Predict 0: %.2f' % ((cm[0,0]+cm[1,0])/total*100))\n",
    "                #print('Predict 1: %.2f' % ((cm[0,1]+cm[1,1])/total*100))\n",
    "                f1 = (f1_score(Y_test, predictY, average=\"micro\"))\n",
    "                prec= (precision_score(Y_test, predictY, average=\"micro\"))\n",
    "                recall = (recall_score(Y_test, predictY, average=\"micro\"))\n",
    "                print('\\nf1_score: %.2f' % (f1*100))\n",
    "                print('precision_score: %.2f'% (prec*100)) # class 0 accuracy\n",
    "                print('recall_score: %.2f' % (recall*100)) # class 1 accuracy\n",
    "                print('\\n%.2f %.2f %.2f %.2f\\n' %(prec*100, recall*100, f1*100, accu*100))\n",
    "                print()\n",
    "                del m3\n",
    "                gc.collect()\n",
    "                \n",
    "                #========================================================================================\n",
    "                # PREDICT & ACCURACY\n",
    "                #========================================================================================\n",
    "                if mode==0:\n",
    "                    print('\\n\\n******************* M5 -sequence RNN *******************')  \n",
    "                elif mode==1:\n",
    "                    print('\\n\\n******************* M6 - Embedding RNN *******************')\n",
    "                y_test_activity_orig = y_test_activity_orig.reshape((-1,))\n",
    "                padding_idx = np.where(y_test_activity_orig==-1)\n",
    "                y_test_activity_orig = np.delete(y_test_activity_orig, padding_idx)\n",
    "                #print('y_test_activity_orig', y_test_activity_orig)\n",
    "                predict_orig = model.predict(X_test_activity_orig)\n",
    "                #print('predict_orig',predict_orig)\n",
    "                predictY_orig = np.argmax(predict_orig, axis=2)\n",
    "                #print('predictY_orig',predictY_orig)\n",
    "                predictY_orig = predictY_orig.reshape((-1,))\n",
    "                predictY_orig = np.delete(predictY_orig, padding_idx)\n",
    "                #print('predictY_orig',predictY_orig)\n",
    "                #print('len(predictY_orig)', len(predictY_orig))\n",
    "                corrects = np.nonzero(predictY_orig == y_test_activity_orig) #.reshape((-1,))\n",
    "                accu = len(corrects[0])/len(y_test_activity_orig)\n",
    "                print(\"Total test accuracy : %.2f\" % (accu*100))\n",
    "                print('\\nConfusion Matrix')\n",
    "                cm = confusion_matrix(y_test_activity_orig, predictY_orig)\n",
    "                print(cm)\n",
    "                f1 = f1_score(y_test_activity_orig, predictY_orig, average=\"micro\")\n",
    "                prec = precision_score(y_test_activity_orig, predictY_orig, average=\"micro\")\n",
    "                recall = recall_score(y_test_activity_orig, predictY_orig, average=\"micro\")\n",
    "                print('\\nf1_score: %.2f' % (f1*100))\n",
    "                print('precision_score: %.2f'% (prec*100)) # class 0 accuracy\n",
    "                print('recall_score: %.2f' % (recall*100)) # class 1 accuracy\n",
    "                total = sum(cm[0])+sum(cm[1])\n",
    "                print('Predict 0: %.2f' % ((cm[0,0]+cm[1,0])/total*100))\n",
    "                print('Predict 1: %.2f' % ((cm[0,1]+cm[1,1])/total*100))\n",
    "                print('\\n%.2f %.2f %.2f %.2f\\n' %(prec*100, recall*100, f1*100, accu*100))\n",
    "                print()\n",
    "                del model\n",
    "            \n",
    "        #print('\\n******************',emb_std, 'M3_runT, average :',m3_runT, sum(m3_runT)/len(m3_runT),'******************')\n",
    "        #print('******************',seq_std, 'M5_runT, average :', m5_runT, sum(m5_runT)/len(m5_runT),'******************\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepTriage",
   "language": "python",
   "name": "rtdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
